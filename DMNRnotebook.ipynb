{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kids Hobby Prediction Dataset**\n",
    "\n",
    "**1.Problem**\n",
    "\n",
    "Children often find themselves at a crossroads when it comes to\n",
    "discovering their passions or hobbies, be it in academics, arts, or\n",
    "sports. Recognizing the importance of guiding children towards\n",
    "activities they are passionate about; we have curated a dataset obtained\n",
    "through surveys conducted with parents. This dataset compiles valuable\n",
    "information about children’s preferences, enabling the creation of a\n",
    "classification model aimed at predicting kids’ hobbies.\n",
    "\n",
    "In this exploratory journey, we delve into the dataset to uncover\n",
    "patterns and insights that can assist parents in understanding their\n",
    "child’s inclinations better. Through the application of clustering\n",
    "techniques, we aim to categorize children based on number of columns\n",
    "(Fav_sub , Scholarship, etc..). The ultimate goal is to provide parents\n",
    "with meaningful recommendations, fostering an environment where children\n",
    "can thrive in activities that genuinely resonate with their interests.\n",
    "\n",
    "By analyzing this dataset, we hope to offer insightful information that\n",
    "will aid parents in guiding their kids toward the right hobbies.\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "**2.Data Mining Task**\n",
    "\n",
    "The data mining task at hand revolves around predicting kids’ hobbies\n",
    "based on a dataset named “Hobby_Data,” obtained through specific\n",
    "questions posed to their parents regarding preferences, capabilities,\n",
    "and achievements. This dataset serves as the foundation for two primary\n",
    "data mining tasks: classification and clustering. In the classification\n",
    "process, the goal is to train a machine learning model to be capable of\n",
    "accurately predicting a child’s hobby as either “academic,” “art,” or\n",
    "“sports.” This requires using the data collected from parents to\n",
    "establish patterns and relationships that guide the model in making\n",
    "accurate predictions. Concurrently, the clustering process involves\n",
    "partitioning the dataset into meaningful clusters, grouping together\n",
    "children with similar characteristics or preferences. Through these dual\n",
    "approaches, the data mining task aims to develop a robust predictive\n",
    "model capable of accurately categorizing children’s hobbies in order to\n",
    "uncover the inherent structures, patterns, and associations within the\n",
    "dataset, contributing to a deeper understanding of the diverse interests\n",
    "and engagement levels of the young population in academic, artistic, and\n",
    "sports-related activities.\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "**3.Data**\n",
    "\n",
    "Following the selection of our data set (“Hobby_Data”) which predicts\n",
    "kids’ hobbies, that was collected by asking their parents specific\n",
    "questions about their kid’s preferences, capabilities, and achievements.\n",
    "To help us train the machine to predict the kid’s hobby. We will begin\n",
    "to preprocess and analyze the data.\n",
    "\n",
    "The source:\n",
    "https://www.kaggle.com/datasets/abtabm/hobby-prediction-basic Number of\n",
    "objects: 1601 Number of attributes: 14\n",
    "\n",
    "**Attribute**\n",
    "\n",
    "| **Attribute name**     | **Description**                                               | **Data type** |\n",
    "|------------------|--------------------------------------------|------------|\n",
    "| Olympiad_Participation | Has your child participated in any Science/Maths              | Boolean       |\n",
    "| Scholarship            | Has he/she received any scholarship?                          | Boolean       |\n",
    "| School                 | Love’s going to school?                                       | Boolean       |\n",
    "| Fav_sub                | What is his/her favorite subject?                             | Categorical   |\n",
    "| Projects               | Has done any projects under academics before?                 | Boolean       |\n",
    "| Grasp_pow              | His/Her Grasping power (1-6)                                  | Ordinal       |\n",
    "| Time_sprt              | How much time does he/she spend playing outdoor/indoor games? | Ordinal       |\n",
    "| Medals                 | Medals won in Sports?                                         | Boolean       |\n",
    "| Career_sprt            | Want’s to pursue his/her career in sports?                    | Boolean       |\n",
    "| Act_sprt               | Regular in his/her sports activities?                         | Boolean       |\n",
    "| Fant_arts              | Love creating fantasy paintings?                              | Boolean       |\n",
    "| Won_arts               | Won art competitions?                                         | Ordinal       |\n",
    "| Time_art               | Time utilized in Arts?                                        | Ordinal       |\n",
    "| Predicted Hobby        | predictions for the hobby that the kid wouldl ike             | Categorical   |\n",
    "\n",
    "======= \\>\\>\\>\\>\\>\\>\\> Stashed changes\n",
    "\n",
    "**General information about the data set:**\n",
    "\n",
    "``` {r}\n",
    "str(Hobby_Data)\n",
    "```\n",
    "\n",
    "**samples of raw dataset:**\n",
    "\n",
    "``` {r}\n",
    "sample(Hobby_Data)\n",
    "```\n",
    "\n",
    "**variables distribution:**\n",
    "\n",
    "    In our dataset, numeric variables are not available; instead, we have three ordinal variables. Due to the nature of our data types, certain types of graphs, such as scatter plots and box plots, were not suitable for our analysis.\n",
    "\n",
    "variables distribution of Time_sprt:\n",
    "\n",
    "``` {r}\n",
    "install.packages(\"magrittr\") # install only one time then put this command as comment after installation\n",
    "library(magrittr) ## for pipe operations\n",
    "Hobby_Data$Time_art %>% density() %>% plot(main='variables distribution of Time_art')\n",
    "```\n",
    "\n",
    "In the “Time_art” variable, parents were requested to assess the time\n",
    "their child dedicated to artistic pursuits like painting or paper\n",
    "crafting, using a scale ranging from 1 to 6, where 6 represents the\n",
    "highest level of involvement. It’s worth noting that the concentration\n",
    "of lower ratings at the lower end of the scale (1) is quite pronounced,\n",
    "and this tendency may be attributed to the inherent inclination of\n",
    "children towards physical activities.\n",
    "\n",
    "variables distribution of Time_art:\n",
    "\n",
    "``` {r}\n",
    "Hobby_Data$Time_sprt %>% density() %>% plot(main='variables distribution of Time_sprt')\n",
    "```\n",
    "\n",
    "Parents were requested to assess their children’s involvement in sports\n",
    "on a scale from 1 to 6 within the “Time_sprt” variable. Notably, the\n",
    "most prevalent ranking was 3, suggesting a moderate level of sports\n",
    "participation. It’s interesting to observe that the distribution\n",
    "exhibits a shape akin to a bell curve, indicating that a substantial\n",
    "proportion of children have a genuine love for sports.\n",
    "\n",
    "variables distribution of Grasp_pow:\n",
    "\n",
    "``` {r}\n",
    "Hobby_Data$Grasp_pow %>% density() %>% plot(main='variables distribution of Grasp_pow')\n",
    "```\n",
    "\n",
    "The density graph depicting parents’ ratings of their children’s grasp\n",
    "power, which ranges from 1 to 6, illustrates a trend where the most\n",
    "common rating is level 3, followed by level 4, level 5, level 2, level\n",
    "1, and level 6. This distribution indicates that a substantial\n",
    "proportion of parents believe their children possess grasp power that is\n",
    "average or slightly above average (levels 3 and 4), with fewer children\n",
    "rated at the extremes (levels 1, 2, 5, and 6).\n",
    "\n",
    "variables distribution of the class label ‘Predicted Hobby’:\n",
    "\n",
    "``` {r}\n",
    "install.packages(\"dplyr\") # install only one time then put this command as comment after installation\n",
    "library(dplyr)\n",
    "\n",
    "dataset2 <- Hobby_Data %>% sample_n(1600)\n",
    "table(dataset2$`Predicted Hobby`) %>% pie()\n",
    "tab <- dataset2$`Predicted Hobby` %>% table()\n",
    "precentages <- tab %>% prop.table() %>% round(3) * 100 \n",
    "txt <- paste0(names(tab), '\\n', precentages, '%')\n",
    "pie(tab, labels=txt)\n",
    "```\n",
    "\n",
    "The pie chart illustrates the distribution of the class label ‘Predicted\n",
    "Hobby’. It’s evident that a substantial portion, approximately 43.7%, of\n",
    "the children’s hobbies are academic in nature, indicating a strong\n",
    "interest in educational pursuits. Additionally, 30.8% of the kids are\n",
    "engaged in sports, reflecting a significant inclination towards physical\n",
    "activities. Arts-related hobbies account for 25.6% of the total, the\n",
    "distribution reflects a harmonious blend of hobbies. This balanced\n",
    "distribution not only signifies a variety of interests but also\n",
    "indicates a well-rounded engagement of children in academic, physical,\n",
    "and creative activities.\n",
    "\n",
    "**Statistical measures :**\n",
    "\n",
    "``` {r}\n",
    "find_mode <- function(x) {\n",
    "  u <- unique(x)\n",
    "  tab <- tabulate(match(x, u))\n",
    "  u[tab == max(tab)]\n",
    "}\n",
    "\n",
    "find_mode(Hobby_Data$Time_art)\n",
    "# Updated upstream\n",
    "hist(Hobby_Data$Time_art)\n",
    "\n",
    "#Stashed changes\n",
    "```\n",
    "\n",
    "The histogram makes it evident that the mode is equal to 1, and we\n",
    "believe that the high frequency of parents ranking “1” as the most\n",
    "chosen rank in the “Time_art” variable could be attributed to several\n",
    "factors. It might indicate that a significant number of parents perceive\n",
    "their children’s involvement in art activities as relatively low,\n",
    "possibly due to time constraints, academic priorities, or a limited\n",
    "interest in art. Alternatively, it could reflect that parents value a\n",
    "more balanced approach to their children’s activities, with a variety of\n",
    "interests and responsibilities sharing their time. This trend could also\n",
    "result from a cultural or educational emphasis on other subjects and\n",
    "extracurricular activities that compete for a child’s time, potentially\n",
    "leading to a lower ranking for art-related activities.\n",
    "\n",
    "``` {r}\n",
    "find_mode(Hobby_Data$Grasp_pow)\n",
    "hist(Hobby_Data$Grasp_pow)\n",
    "```\n",
    "\n",
    "Rank 3, in this context, may have been the most chosen rank because it\n",
    "likely represents an average or moderate level of grasp power. Parents\n",
    "may have assessed their children’s grasp power as neither exceptionally\n",
    "strong (rank 5 or 6) nor particularly weak (rank 1 or 2), resulting in\n",
    "the preference for the middle-ranking option. This choice could reflect\n",
    "a perception that their children’s grasp power falls within a typical or\n",
    "expected range, making it the most common rating.\n",
    "\n",
    "``` {r}\n",
    "find_mode(Hobby_Data$Time_sprt)\n",
    "hist(Hobby_Data$Time_sprt)\n",
    "```\n",
    "\n",
    "As shown in the histogram the rank 3 on a scale of 1 to 6 was likely the\n",
    "most chosen rank for assessing children’s involvement in sports because\n",
    "it represents a balanced middle ground. Parents may have perceived rank\n",
    "3 as indicating that their children are moderately involved in sports,\n",
    "not excessively committed or disinterested. This middle-of-the-road\n",
    "ranking reflects a common perspective that many parents may hold,\n",
    "considering that extreme rankings, such as 1 or 6, might suggest either\n",
    "a lack of involvement or an excessive focus on sports, which may not\n",
    "align with their perception of their child’s overall well-rounded\n",
    "development.\n",
    "\n",
    "The histogram graph for “Time_art” variable\n",
    "\n",
    "``` {r}\n",
    "hist(Hobby_Data$Time_art)\n",
    "```\n",
    "\n",
    "The histogram makes it evident that the mode is equal to 1, and we\n",
    "believe that the high frequency of parents ranking “1” as the most\n",
    "chosen rank in the “Time_art” variable could be attributed to several\n",
    "factors. It might indicate that a significant number of parents perceive\n",
    "their children’s involvement in art activities as relatively low,\n",
    "possibly due to time constraints, academic priorities, or a limited\n",
    "interest in art. Alternatively, it could reflect that parents value a\n",
    "more balanced approach to their children’s activities, with a variety of\n",
    "interests and responsibilities sharing their time. This trend could also\n",
    "result from a cultural or educational emphasis on other subjects and\n",
    "extracurricular activities that compete for a child’s time, potentially\n",
    "leading to a lower ranking for art-related activities.\n",
    "\n",
    "``` {r}\n",
    "find_mode(Hobby_Data$Grasp_pow)\n",
    "```\n",
    "\n",
    "The histogram graph for “Grasp_pow” variable\n",
    "\n",
    "``` {r}\n",
    "hist(Hobby_Data$Grasp_pow)\n",
    "```\n",
    "\n",
    "Rank 3, in this context, may have been the most chosen rank because it\n",
    "likely represents an average or moderate level of grasp power. Parents\n",
    "may have assessed their children’s grasp power as neither exceptionally\n",
    "strong (rank 5 or 6) nor particularly weak (rank 1 or 2), resulting in\n",
    "the preference for the middle-ranking option. This choice could reflect\n",
    "a perception that their children’s grasp power falls within a typical or\n",
    "expected range, making it the most common rating.\n",
    "\n",
    "``` {r}\n",
    "find_mode(Hobby_Data$Time_sprt)\n",
    "```\n",
    "\n",
    "The histogram graph for “Time_sprt” variable\n",
    "\n",
    "``` {r}\n",
    "hist(Hobby_Data$Time_sprt)\n",
    "```\n",
    "\n",
    "As shown in the histogram the rank 3 on a scale of 1 to 6 was likely the\n",
    "most chosen rank for assessing children’s involvement in sports because\n",
    "it represents a balanced middle ground. Parents may have perceived rank\n",
    "3 as indicating that their children are moderately involved in sports,\n",
    "not excessively committed or disinterested. This middle-of-the-road\n",
    "ranking reflects a common perspective that many parents may hold,\n",
    "considering that extreme rankings, such as 1 or 6, might suggest either\n",
    "a lack of involvement or an excessive focus on sports, which may not\n",
    "align with their perception of their child’s overall well-rounded\n",
    "development.\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "**5.Data preprocessing**\n",
    "\n",
    "**#1#Data cleaning:**\n",
    "\n",
    "During the data cleaning stage, finding and fixing faults,\n",
    "inconsistencies, and errors in a dataset helps it be more reliable and\n",
    "of higher quality for analysis and modeling. There are methods for\n",
    "handling missing values, detecting outliers, resolving inconsistencies,\n",
    "and standardizing formats.\n",
    "\n",
    "view Dataset”Hobby_Data”\n",
    "\n",
    "``` {r}\n",
    "View(Hobby_Data)\n",
    "```\n",
    "\n",
    "check missing value :\n",
    "\n",
    "``` {r}\n",
    "is.na(Hobby_Data)\n",
    "```\n",
    "\n",
    "find the total null values in the dataset:\n",
    "\n",
    "``` {r}\n",
    "sum(is.na(Hobby_Data))\n",
    "```\n",
    "\n",
    "This stage involves checking and deleting for null and missing values\n",
    "because they might have a significant impact on the data and cause\n",
    "errors and negative effects in subsequent steps.We simply looked for\n",
    "missing values, and there are not missing values in our dataset.\n",
    "According to our investigation, the dataset does not contain any\n",
    "outliers since it doesn’t have a numerical data type. Additionally,\n",
    "there are no inconsistent values or other errors.\n",
    "\n",
    "Dataset after cleaning step:\n",
    "\n",
    "``` {r}\n",
    "sample(Hobby_Data)\n",
    "```\n",
    "\n",
    "**#2#Second:Encoding:** This step includes a Converting categorical or\n",
    "non-numeric data into a numerical format, which is necessary for\n",
    "compatibility with subsequent steps in preprocessing.\\[1\\]\n",
    "\n",
    "``` {r}\n",
    "Hobby_Data$Olympiad_Participation = factor(Hobby_Data$Olympiad_Participation,levels = c(\"No\", \"Yes\"), labels = c(0, 1))\n",
    "Hobby_Data$Scholarship = factor(Hobby_Data$Scholarship , levels = c(\"No\", \"Yes\"), labels = c(0, 1))\n",
    "Hobby_Data$School = factor(Hobby_Data$School, levels = c(\"No\", \"Yes\"), labels = c(0, 1))\n",
    "Hobby_Data$Projects = factor(Hobby_Data$Projects, levels = c(\"No\", \"Yes\"), labels = c(0, 1))\n",
    "Hobby_Data$Medals = factor(Hobby_Data$Medals, levels = c(\"No\", \"Yes\"), labels = c(0, 1))\n",
    "Hobby_Data$Career_sprt = factor(Hobby_Data$Career_sprt, levels = c(\"No\", \"Yes\"), labels = c(0, 1))\n",
    "Hobby_Data$Act_sprt = factor(Hobby_Data$Act_sprt, levels = c(\"No\", \"Yes\"), labels = c(0, 1))\n",
    "Hobby_Data$Fant_arts = factor(Hobby_Data$Fant_arts, levels = c(\"No\", \"Yes\"), labels = c(0, 1))\n",
    "Hobby_Data$Won_arts = factor(Hobby_Data$Won_arts, levels = c(\"No\", \"Maybe\", \"Yes\"), labels = c(0, 2, 1))\n",
    "Hobby_Data$Fav_sub = factor(Hobby_Data$Fav_sub, levels = c(\"Science\", \"Mathematics\", \"History/Geography\", \"Any language\"), labels = c(1, 2, 3, 4))\n",
    "Hobby_Data$`Predicted Hobby` <- factor(Hobby_Data$`Predicted Hobby`, levels = c(\"Academics\", \"Arts\", \"Sports\"), labels = c(1, 2, 3))\n",
    "```\n",
    "\n",
    "Dataset after Encoding :\n",
    "\n",
    "``` {r}\n",
    "sample(Hobby_Data)\n",
    "```\n",
    "\n",
    "**#3# Normalization and Discetization:**\n",
    "\n",
    "We don’t need to use normalization and discetization in our dataset.\n",
    "Since our dataset doesn’t have numeric attributes and normalization\n",
    "involves mathematical operations, which can result in meaningless values\n",
    "and errors, Also, applying discretization leads to a loss of information\n",
    "and creates intervals and relationships that don’t exist between\n",
    "values.\\[2\\]\n",
    "\n",
    "**#4#Feature Selection:**\n",
    "\n",
    "To improve the accuracy of our predictions for the target class\n",
    "“Predicted Hobby” and decrease the processing time of our classifier, we\n",
    "will utilize feature selection techniques. These techniques enable us to\n",
    "eliminate redundant or irrelevant attributes from the dataset, resulting\n",
    "in a more concise subset of features that provide the most valuable\n",
    "information for our predictions.\\[3\\]\n",
    "\n",
    "Specifically, we will use two feature selection methods: Rank Features\n",
    "by Importance and Feature Selection Using Recursive Feature Elimination\n",
    "(RFE).\n",
    "\n",
    "1.**Rank Features by Importance:**\n",
    "\n",
    "This method used in the code helps us determine which features are most\n",
    "important for predicting the “Predicted Hobby” class label in the\n",
    "dataset. It utilizes the Random Forest algorithm, known for its accurate\n",
    "prediction capabilities. This method calculates the importance of each\n",
    "feature by assessing its contribution to the overall accuracy of the\n",
    "predictions. By ranking the features based on their importance, we can\n",
    "identify the ones that have the greatest influence on determining\n",
    "hobbies.\n",
    "\n",
    "Ensure the results are repeatable by setting a seed:\n",
    "\n",
    "``` {r}\n",
    "set.seed(7)\n",
    "```\n",
    "\n",
    "Load the necessary libraries:\n",
    "\n",
    "``` {r}\n",
    "install.packages(\"caret\")\n",
    "install.packages(\"randomForest\")\n",
    "library(caret)\n",
    "library(randomForest)\n",
    "```\n",
    "\n",
    "Separate the predictors and the class label:\n",
    "\n",
    "``` {r}\n",
    "predictors <- Hobby_Data[, -14]  # Excluding the class label (Predicted Hobby)\n",
    "class_label <- Hobby_Data$`Predicted Hobby`\n",
    "```\n",
    "\n",
    "Train a Random Forest model:\n",
    "\n",
    "``` {r}\n",
    "model <- randomForest(predictors, class_label, importance = TRUE)\n",
    "```\n",
    "\n",
    "Get the variable importance:\n",
    "\n",
    "``` {r}\n",
    "importance <- importance(model)\n",
    "```\n",
    "\n",
    "Rank the features by importance:\n",
    "\n",
    "``` {r}\n",
    "ranked_features <- sort(importance[, \"MeanDecreaseGini\"], decreasing = TRUE)\n",
    "```\n",
    "\n",
    "Print the ranked features:\n",
    "\n",
    "``` {r}\n",
    "print(ranked_features)\n",
    "\n",
    "barplot(ranked_features, horiz = TRUE, las = 1, main = \"Kids Hobby Variable Importance Ranking\")\n",
    "```\n",
    "\n",
    "2.**Feature Selection Using RFE:**\n",
    "\n",
    "The Recursive Feature Elimination (RFE) method with Random Forest is a\n",
    "technique used to select the most important features for accurate\n",
    "predictions . It iteratively eliminates less relevant features,\n",
    "retraining the model at each step to evaluate performance. By focusing\n",
    "on the most informative features, RFE improves reduces complexity, and\n",
    "enhances prediction accuracy. It is particularly effective with Random\n",
    "Forest due to its ability to handle complex relationships and\n",
    "high-dimensional data. RFE helps identify the most important attributes\n",
    "associated with the target variable, enabling the creation of more\n",
    "efficient and accurate models.\n",
    "\n",
    "Ensure the results are repeatable by setting a seed:\n",
    "\n",
    "``` {r}\n",
    "set.seed(7)\n",
    "```\n",
    "\n",
    "Load the necessary libraries:\n",
    "\n",
    "``` {r}\n",
    "library(caret)\n",
    "```\n",
    "\n",
    "Define the control parameters for RFE using random forest selection\n",
    "function:\n",
    "\n",
    "``` {r}\n",
    "control <- rfeControl(functions = rfFuncs, method = \"cv\", number = 10)\n",
    "```\n",
    "\n",
    "Extract the predictor variables from Hobby_Data:\n",
    "\n",
    "``` {r}\n",
    "predictors <- Hobby_Data[, -ncol(Hobby_Data)]\n",
    "```\n",
    "\n",
    "Convert the outcome variable to a factor:\n",
    "\n",
    "``` {r}\n",
    "outcome <- as.factor(Hobby_Data$`Predicted Hobby`)\n",
    "```\n",
    "\n",
    "Run the RFE algorithm:\n",
    "\n",
    "``` {r}\n",
    "results <- rfe(predictors, outcome, sizes = 1:ncol(Hobby_Data), rfeControl = control)\n",
    "```\n",
    "\n",
    "Summarize the results:\n",
    "\n",
    "``` {r}\n",
    "print(results)\n",
    "```\n",
    "\n",
    "List the chosen features selected by RFE:\n",
    "\n",
    "``` {r}\n",
    "predictors(results)\n",
    "```\n",
    "\n",
    "Plot the results:\n",
    "\n",
    "``` {r}\n",
    "plot(results, type = c(\"g\", \"o\"))\n",
    "```\n",
    "\n",
    "3.**Removing Irrelevant Columns:**\n",
    "\n",
    "By considering both Recursive Feature Elimination (RFE) and Rank By\n",
    "Importance, we can make informed decisions about feature relevance and\n",
    "impact on the model. In this case, the columns “School,” “Medals” should\n",
    "be deleted as they have lower importance scores compared to the selected\n",
    "variables. Removing these columns simplifies the model and reduces\n",
    "dimensionality, eliminating potential noise and irrelevant information\n",
    "that could hinder accurate predictions.\n",
    "\n",
    "Remove the specified columns from the Hobby_Kids dataset:\n",
    "\n",
    "``` {r}\n",
    "Hobby_Data <- Hobby_Data[, !(colnames(Hobby_Data) %in% c(\"School\", \"Medals\"))]\n",
    "```\n",
    "\n",
    "sample of our dataset after finishing from pre-processing:\n",
    "\n",
    "``` {r}\n",
    "sample(Hobby_Data)\n",
    "```\n",
    "\n",
    "we can see that the two columns are deleted(“School”, “Medals”).\n",
    "\n",
    "**Balanced Data**\n",
    "\n",
    "``` {r}\n",
    "# Calculate class imbalance\n",
    "class_imbalance <- max(prop.table(table(Hobby_Data$`Predicted Hobby`))) - min(prop.table(table(Hobby_Data$`Predicted Hobby`)))\n",
    "\n",
    "# Print the result\n",
    "print(class_imbalance)\n",
    "```\n",
    "\n",
    "if class_imbalance is close to 0, it suggests that the proportions of\n",
    "different classes are relatively similar, indicating a balanced dataset.\n",
    "Conversely, if class_imbalance is larger, it suggests a more significant\n",
    "imbalance between the classes. The calculated class imbalance value of\n",
    "0.1805122 suggests that the distribution of classes in the\n",
    "“Predicted_Hobby” column of the dataset is relatively balanced. The\n",
    "class imbalance is a measure of the difference between the proportions\n",
    "of the most prevalent and least prevalent classes. In this case, the\n",
    "value is close to 0, indicating that there is a minimal difference\n",
    "between the proportions of different classes. A lower-class imbalance\n",
    "value is generally desirable, as it signifies a more even distribution\n",
    "of instances across classes, which can be beneficial for the training\n",
    "and performance of machine learning models.\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "**5.Data Mining Technique**\n",
    "\n",
    "**Classification Technique:**\n",
    "\n",
    "In the realm of classification for data mining, the selection of\n",
    "attributes or features plays a pivotal role in building robust models.\n",
    "This process involves identifying the most informative attributes that\n",
    "aid in accurately predicting the target variable. Simultaneously, we\n",
    "choose three different split sizes: (“90%”, “10%”), (“80%”, “20%”), and\n",
    "(“70%”, “30%”). These varying sizes are selected to provide insight into\n",
    "the model’s performance with different amounts of data, which is vital\n",
    "for detecting unique patterns and confirming the model’s consistency in\n",
    "various situations.\n",
    "\n",
    "1-Information Gain (IG): Information Gain serves as a criterion to\n",
    "determine the relevance of attributes by measuring the amount of\n",
    "information provided by each attribute. Higher Information Gain suggests\n",
    "attributes that are more influential in the classification process.\n",
    "\n",
    "R Packages and Methods: R Package: rpart Method: Decision Tree\n",
    "(implemented via rpart package) Procedure: Using the rpart package in R,\n",
    "the decision tree model is built by partitioning the dataset based on\n",
    "the attribute with the highest Information Gain. The parms = list(split\n",
    "= “information”) parameter signifies the use of Information Gain for\n",
    "splitting.\n",
    "\n",
    "2-Gini Index: Gini Index, employed in decision tree algorithms like\n",
    "CART, quantifies the probability of misclassifying a randomly chosen\n",
    "element based on the distribution of labels in a subset. It identifies\n",
    "attributes that minimize misclassification and contribute significantly\n",
    "to the classification process.\n",
    "\n",
    "R Packages and Methods: R Package: rpart Method: Decision Tree (using\n",
    "rpart package) Procedure: Utilizing the rpart package, the decision tree\n",
    "model is constructed based on the Gini Index. The Gini Index helps\n",
    "determine attribute importance and guides the splitting of nodes in the\n",
    "tree for optimal classification.\n",
    "\n",
    "3-Gain Ratio: The Gain Ratio is an enhancement of Information Gain that\n",
    "addresses the bias towards attributes with a larger number of values. It\n",
    "normalizes the Information Gain by considering the intrinsic information\n",
    "of each attribute.\n",
    "\n",
    "R Packages and Methods: R Package: C50 Method: Decision Tree (via C50\n",
    "package) Procedure: The C50 package in R facilitates the implementation\n",
    "of the Gain Ratio within a decision tree framework. By employing this\n",
    "package, the model accounts for attribute selection based on the Gain\n",
    "Ratio, ensuring a more balanced evaluation of attributes. In these\n",
    "techniques, the choice of methodology—Decision Trees based on different\n",
    "attribute selection measures—provides a versatile approach to\n",
    "classification. The selection of the most appropriate method often\n",
    "involves experimentation and comparative analysis to determine which\n",
    "attribute selection measure yields the most accurate and interpretable\n",
    "models for the given dataset.\n",
    "\n",
    "Each method allows for a unique perspective on attribute importance,\n",
    "contributing to the overall accuracy and robustness of the\n",
    "classification model. Experimentation with these techniques helps in\n",
    "understanding the impact of attribute selection on model performance,\n",
    "providing valuable insights into feature importance and classification\n",
    "accuracy.\n",
    "\n",
    "**Clustering Techniques:**\n",
    "\n",
    "since the k-mean does not suitable to our data because the k-means\n",
    "algorithm is not applicable to categorical data, because it relies on\n",
    "the Euclidean distance metric tomeasure the similarity between data\n",
    "points.and other things we mention later,we decide to use\n",
    "k-medoids(PAM). Unlike k-means, k-medoids does not rely on the mean as a\n",
    "representative centroid but employs medoids, which are actual data\n",
    "points within the clusters, and the algorithm defines clusters based on\n",
    "partitioning around medoids. This feature makes k-medoids particularly\n",
    "suitable for categorical data.\n",
    "\n",
    "since clustering apply with a data set without ground truth we remove it\n",
    "before we start to make clustering, first we convert type from factor\n",
    "columns to numeric then scaled all columns, then we make validation to\n",
    "determined how many clusters by using (Silhouette coefficient,Elbow\n",
    "method, average silhouette) method\n",
    "\n",
    "second we apply PAM for 3 number of cluster with calculation of (BCubed\n",
    "recall, BCubed precision,wws, average silhouette width)\n",
    "\n",
    "third after we apply PAM and some methods for every number of cluster,\n",
    "we compare the result and chose k=3 to be the best number of cluster\n",
    "based on our calculation and visuals.\n",
    "\n",
    "Packages Used: ggplot2: Enables visually appealing and customizable\n",
    "plots, widely used for data visualization in R. magrittr: Facilitates\n",
    "writing readable code by employing the pipe operator (%\\>%) for data\n",
    "manipulation operations. dplyr: Offers tools for data manipulation and\n",
    "transformation, providing functions for filtering, selecting, and\n",
    "arranging data. factoextra: Useful for extracting and visualizing\n",
    "information from principal component analysis (PCA) and clustering\n",
    "results. cluster: Provides functions for clustering algorithms such as\n",
    "k-means. k-medoids, hierarchical clustering .\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "**6.Implement Classification and clustering**\n",
    "\n",
    "**#1#Classification**\n",
    "\n",
    "After preprocessing, we will proceed to the classification step. In this\n",
    "phase, as part of supervised learning, we will apply a classification\n",
    "algorithm to assign each data point into predefined categories based on\n",
    "its attributes. This involves selecting the most relevant features that\n",
    "have been cleaned and formatted during preprocessing. The selected model\n",
    "will then learn from training data, enabling it to predict the category\n",
    "of new, unseen data accurately. This step is essential for making\n",
    "informed decisions or predictions based on the data.\n",
    "\n",
    "We implement a decision tree on the dataset, which has been partitioned\n",
    "into Training and Test sets using the percentage split method. This\n",
    "method ensures that each subset is a randomized, representative sample\n",
    "of the entire dataset, thus minimizing bias and enabling consistent\n",
    "model performance evaluation. We choose three different split sizes:\n",
    "(“90%”, “10%”), (“80%”, “20%”), and (“70%”, “30%”). These varying sizes\n",
    "are selected to provide insight into the model’s performance with\n",
    "different amounts of data, which is vital for detecting unique patterns\n",
    "and confirming the model’s consistency in various situations.\n",
    "\n",
    "In the final steps, we utilize data visualization tools to create visual\n",
    "representations of our decision trees. Additionally, we conduct an\n",
    "exhaustive evaluation of the model, employing a “Confusion Matrix” to\n",
    "illustrate the outcomes clearly.\n",
    "\n",
    "**Information Gain**\n",
    "\n",
    "Information Gain is particularly useful when dealing with categorical\n",
    "target variables using Information Gain for the initial partitioning of\n",
    "a dataset when building a decision tree It’s based on the concept of\n",
    "entropy and aims to maximize the homogeneity of subsets after each\n",
    "split. This approach helps create an effective decision tree by\n",
    "selecting features that provide the most information for predicting the\n",
    "target variable.\\[4\\]\n",
    "\n",
    "**1-Information Gain(70%,30%)**\n",
    "\n",
    "``` {r}\n",
    "# Load the required packages\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "library(caret)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set.seed(1234)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "ind <- sample(2, nrow(Hobby_Data), replace=TRUE, prob=c(0.7, 0.3))\n",
    "trainData <- Hobby_Data[ind == 1,]\n",
    "testData <- Hobby_Data[ind == 2,]\n",
    "\n",
    "# Define the formula for the decision tree\n",
    "myFormula <- `Predicted Hobby` ~ Scholarship + Fav_sub + Projects + Grasp_pow + Time_sprt + Career_sprt + Act_sprt + Fant_arts + Won_arts + Time_art + Olympiad_Participation\n",
    "\n",
    "# Create the decision tree model with the \"information\" splitting criterion\n",
    "Hobby_Data_ctree <- rpart(myFormula, data = trainData, method = \"class\", parms = list(split = \"information\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the decision tree\n",
    "print(Hobby_Data_ctree)\n",
    "\n",
    "# Plot the decision tree\n",
    "\n",
    "rpart.plot(Hobby_Data_ctree)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Decision Tree Analysis Using Information gain(70/30):**\n",
    "\n",
    "In Frist Tree ,we devide dataset into training set and test set with\n",
    "size(%70,%30) respectively. As you can see in the figure, the root node\n",
    "(“Career_sprt”) serves as the starting point for the classification\n",
    "process since have the heights Gain. The dataset has a distribution of\n",
    "approximately 42.47% for class 1(“Academics”), 25.92% for class\n",
    "2(“Arts”), and 31.61% for class 3 (“Sports”).\n",
    "\n",
    "The tree further branches based on the values of the “Career_sprt” if\n",
    "equal 0, tree branches based on”Won_arts” ,and majority of instances\n",
    "fall into “Acadimcs”, constituting 63.93% , if”Won_arts” equal 0 or2 In\n",
    "this case the tree terminates with a leaf node indicating a high\n",
    "probability 89.08% for “Acadimcs”.else the instances are classified\n",
    "based on “Fant_arts” into “Arts” with a probability 88%,if equal 0 the\n",
    "instances are classified into “Academics” with a probability of 73%\n",
    ",else the instances are classified into “Arts” with a high probability\n",
    "96%, if “Career_sprt” is 1, the tree further branches based on the\n",
    "“Fant_arts” into “Sports” with a probability 80%. If “Fant_arts” is 1 ,\n",
    "there is another split based on the “Time_art” feature into “Arts” with\n",
    "a probability 49%. If “Time_art” is greater than or equal to 3, the\n",
    "instances are classified into “Arts” with a probability of 87% . On the\n",
    "other hand, if “Time_art” is less than 3, the instances are classified\n",
    "into “Sports” with a probability of 65% .If “Fant_arts” not equal 1 ,\n",
    "the instances are classified into “Sports” with a high probability 95%,\n",
    "as indicated by the leaf node.\n",
    "\n",
    "**First Confusion matrix**\n",
    "\n",
    "``` {r}\n",
    "# Predict on the test data\n",
    "testPred <- predict(Hobby_Data_ctree, newdata = testData, type = 'class')\n",
    "\n",
    "# Check the accuracy of the model\n",
    "accuracy <- sum(testPred == testData$`Predicted Hobby`) / nrow(testData) * 100\n",
    "cat('Accuracy:', accuracy, '\\n')\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix <- table(Actual = testData$`Predicted Hobby`, Predicted = testPred)\n",
    "\n",
    "# Calculate precision for each class\n",
    "precision_class_1 <- conf_matrix[1, 1] / sum(conf_matrix[1, ])\n",
    "precision_class_2 <- conf_matrix[2, 2] / sum(conf_matrix[2, ])\n",
    "precision_class_3 <- conf_matrix[3, 3] / sum(conf_matrix[3, ])\n",
    "\n",
    "# Calculate sensitivity for each class\n",
    "sensitivity_class_1 <- conf_matrix[1, 1] / sum(conf_matrix[1, ])\n",
    "sensitivity_class_2 <- conf_matrix[2, 2] / sum(conf_matrix[2, ])\n",
    "sensitivity_class_3 <- conf_matrix[3, 3] / sum(conf_matrix[3, ])\n",
    "\n",
    "# Calculate specificity for each class\n",
    "specificity_class_1 <- sum(diag(conf_matrix[-1, -1])) / sum(conf_matrix[-1, ])\n",
    "specificity_class_2 <- sum(conf_matrix[c(1, 3), c(1, 3)]) / sum(conf_matrix[c(1, 3), ])\n",
    "specificity_class_3 <- sum(conf_matrix[c(1, 2), c(1, 2)]) / sum(conf_matrix[c(1, 2), ])\n",
    "\n",
    "# Calculate macro-average sensitivity\n",
    "macro_avg_sensitivity <- (sensitivity_class_1 + sensitivity_class_2 + sensitivity_class_3) / 3\n",
    "\n",
    "# Calculate macro-average specificity\n",
    "macro_avg_specificity <- (specificity_class_1 + specificity_class_2 + specificity_class_3) / 3\n",
    "\n",
    "# Calculate macro-average precision\n",
    "macro_avg_precision <- (precision_class_1 + precision_class_2 + precision_class_3) / 3\n",
    "\n",
    "# Print macro-average sensitivity\n",
    "cat('Average Sensitivity:', macro_avg_sensitivity, '\\n')\n",
    "\n",
    "# Print macro-average specificity\n",
    "cat('Average Specificity:', macro_avg_specificity, '\\n')\n",
    "\n",
    "# Print macro-average precision\n",
    "cat('Average Precision:', macro_avg_precision, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print precision for each class\n",
    "cat('Precision for Class 1:', precision_class_1, ' \\n')\n",
    "\n",
    "cat('Precision for Class 2:', precision_class_2, '  \\n')\n",
    "cat('Precision for Class 3:', precision_class_3, ' \\n')\n",
    "\n",
    "# Print sensitivity for each class\n",
    "cat('Sensitivity for Class 1:', sensitivity_class_1, '\\n')\n",
    "cat('Sensitivity for Class 2:', sensitivity_class_2, '\\n')\n",
    "cat('Sensitivity for Class 3:', sensitivity_class_3, '\\n')\n",
    "\n",
    "# Print specificity for each class\n",
    "cat('Specificity for Class 1:', specificity_class_1, '\\n')\n",
    "cat('Specificity for Class 2:', specificity_class_2, '\\n')\n",
    "cat('Specificity for Class 3:', specificity_class_3, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**2-Information Gain(80%,20%)**\n",
    "\n",
    "``` {r}\n",
    "library(rpart)\n",
    "\n",
    "library(caTools)\n",
    "library(rpart.plot)\n",
    "library(caret)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set.seed(1234)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "ind <- sample(2, nrow(Hobby_Data), replace=TRUE, prob=c(0.8, 0.2))\n",
    "trainData <- Hobby_Data[ind == 1,]\n",
    "testData <- Hobby_Data[ind == 2,]\n",
    "\n",
    "# Define the formula for the decision tree\n",
    "myFormula <- `Predicted Hobby` ~ Scholarship + Fav_sub + Projects + Grasp_pow + Time_sprt + Career_sprt + Act_sprt + Fant_arts + Won_arts + Time_art + Olympiad_Participation\n",
    "\n",
    "# Create the decision tree model with the \"information\" splitting criterion\n",
    "Hobby_Data_ctree <- rpart(myFormula, data = trainData, method = \"class\", parms = list(split = \"information\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the decision tree\n",
    "print(Hobby_Data_ctree)\n",
    "\n",
    "# Plot the decision tree\n",
    "\n",
    "rpart.plot(Hobby_Data_ctree)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Decision Tree Analysis Using Information gain(80/20):**\n",
    "\n",
    "In Second Tree ,we devide dataset into training set and test set with\n",
    "size(%80,%20) respectively. As you can see in the figure, the root node\n",
    "(“Career_sprt”) serves as the starting point for the classification\n",
    "process since have the heights Gain. The dataset has a distribution of\n",
    "approximately 43%for class1 (“Academics”), 26% for class 2(“Arts”), and\n",
    "31% for class 3 (“Sports”).\n",
    "\n",
    "The tree further branches based on the values of the “Career_sprt” if\n",
    "equal 0, tree branches based on”Won_arts” ,and majority of instances\n",
    "fall into “Acadimcs”, constituting 65% , if”Won_arts” equal 0 or2 In\n",
    "this case the tree terminates with a leaf node indicating a high\n",
    "probability 90% for “Acadimcs”.else the instances are classified based\n",
    "on “Fant_arts”into “Arts” with a probability 88%,if equal 0 the\n",
    "instances are classified into “Academics” with a probability of 72%\n",
    ",else the instances are classified into “Arts” with a high probability\n",
    "96%, if “Career_sprt” is 1, the tree further branches based on the\n",
    "“Fant_arts” into “Sports” with a probability 78%. If “Fant_arts” is 1 ,\n",
    "there is another split based on the “Time_art” into “Arts” with a\n",
    "probability 50%. If “Time_art” is greater than or equal to 3, the\n",
    "instances are classified into “Arts” with a probability of 85% . On the\n",
    "other hand, if “Time_art” is less than 3, the instances are classified\n",
    "into “Sports” with a probability of 58% .If “Fant_arts” If not equal 1 ,\n",
    "the instances are classified into “Sports” with a high probability 94%,\n",
    "as indicated by the leaf node.\n",
    "\n",
    "**Second Confusion matrix**\n",
    "\n",
    "``` {r}\n",
    "\n",
    "# Predict on the test data\n",
    "testPred <- predict(Hobby_Data_ctree, newdata = testData, type = 'class')\n",
    "\n",
    "# Check the accuracy of the model\n",
    "accuracy <- sum(testPred == testData$`Predicted Hobby`) / nrow(testData) * 100\n",
    "cat('Accuracy:', accuracy, '\\n')\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix <- table(Actual = testData$`Predicted Hobby`, Predicted = testPred)\n",
    "\n",
    "# Calculate precision for each class\n",
    "precision_class_1 <- conf_matrix[1, 1] / sum(conf_matrix[1, ])\n",
    "precision_class_2 <- conf_matrix[2, 2] / sum(conf_matrix[2, ])\n",
    "precision_class_3 <- conf_matrix[3, 3] / sum(conf_matrix[3, ])\n",
    "\n",
    "# Calculate sensitivity for each class\n",
    "sensitivity_class_1 <- conf_matrix[1, 1] / sum(conf_matrix[1, ])\n",
    "sensitivity_class_2 <- conf_matrix[2, 2] / sum(conf_matrix[2, ])\n",
    "sensitivity_class_3 <- conf_matrix[3, 3] / sum(conf_matrix[3, ])\n",
    "\n",
    "# Calculate specificity for each class\n",
    "specificity_class_1 <- sum(diag(conf_matrix[-1, -1])) / sum(conf_matrix[-1, ])\n",
    "specificity_class_2 <- sum(conf_matrix[c(1, 3), c(1, 3)]) / sum(conf_matrix[c(1, 3), ])\n",
    "specificity_class_3 <- sum(conf_matrix[c(1, 2), c(1, 2)]) / sum(conf_matrix[c(1, 2), ])\n",
    "\n",
    "# Calculate macro-average sensitivity\n",
    "macro_avg_sensitivity <- (sensitivity_class_1 + sensitivity_class_2 + sensitivity_class_3) / 3\n",
    "\n",
    "# Calculate macro-average specificity\n",
    "macro_avg_specificity <- (specificity_class_1 + specificity_class_2 + specificity_class_3) / 3\n",
    "\n",
    "# Calculate macro-average precision\n",
    "macro_avg_precision <- (precision_class_1 + precision_class_2 + precision_class_3) / 3\n",
    "\n",
    "# Print macro-average sensitivity\n",
    "cat('Average Sensitivity:', macro_avg_sensitivity, '\\n')\n",
    "\n",
    "# Print macro-average specificity\n",
    "cat('Average Specificity:', macro_avg_specificity, '\\n')\n",
    "\n",
    "# Print macro-average precision\n",
    "cat('Average Precision:', macro_avg_precision, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print precision for each class\n",
    "cat('Precision for Class 1:', precision_class_1, ' \\n')\n",
    "\n",
    "cat('Precision for Class 2:', precision_class_2, '  \\n')\n",
    "cat('Precision for Class 3:', precision_class_3, ' \\n')\n",
    "\n",
    "# Print sensitivity for each class\n",
    "cat('Sensitivity for Class 1:', sensitivity_class_1, '\\n')\n",
    "cat('Sensitivity for Class 2:', sensitivity_class_2, '\\n')\n",
    "cat('Sensitivity for Class 3:', sensitivity_class_3, '\\n')\n",
    "\n",
    "# Print specificity for each class\n",
    "cat('Specificity for Class 1:', specificity_class_1, '\\n')\n",
    "cat('Specificity for Class 2:', specificity_class_2, '\\n')\n",
    "cat('Specificity for Class 3:', specificity_class_3, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**3-Information Gain(90%,10%)**\n",
    "\n",
    "``` {r}\n",
    "library(rpart)\n",
    "\n",
    "library(caTools)\n",
    "library(rpart.plot)\n",
    "library(caret)\n",
    "# Set the seed for reproducibility\n",
    "set.seed(1234)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "ind <- sample(2, nrow(Hobby_Data), replace=TRUE, prob=c(0.9, 0.1))\n",
    "trainData <- Hobby_Data[ind == 1,]\n",
    "testData <- Hobby_Data[ind == 2,]\n",
    "\n",
    "# Define the formula for the decision tree\n",
    "myFormula <- `Predicted Hobby` ~ Scholarship + Fav_sub + Projects + Grasp_pow + Time_sprt + Career_sprt + Act_sprt + Fant_arts + Won_arts + Time_art + Olympiad_Participation\n",
    "\n",
    "# Create the decision tree model with the \"information\" splitting criterion\n",
    "Hobby_Data_ctree <- rpart(myFormula, data = trainData, method = \"class\", parms = list(split = \"information\"))\n",
    "\n",
    "\n",
    "\n",
    "# Print the decision tree\n",
    "print(Hobby_Data_ctree)\n",
    "\n",
    "# Plot the decision tree\n",
    "\n",
    "rpart.plot(Hobby_Data_ctree)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Decision Tree Analysis Using Information gain(90/10):**\n",
    "\n",
    "In Third Tree ,we devide dataset into training set and test set with\n",
    "size(%90,%10) respectively. As you can see in the figure, the root node\n",
    "(“Career_sprt”) serves as the starting point for the classification\n",
    "process since have the heights Gain. The dataset has a distribution of\n",
    "approximately 43% for class 1(“Academics”), 25% for class 2(“Arts”), and\n",
    "32% for class 3 (“Sports”).\n",
    "\n",
    "The tree further branches based on the values of the “Career_sprt” if\n",
    "equal 0, tree branches based on”Won_arts” ,and majority of instances\n",
    "fall into “Acadimcs”, constituting 65% , if”Won_arts” equal 0 or 2 In\n",
    "this case the tree terminates with a leaf node indicating a high\n",
    "probability 90% for “Acadimcs”.else the instances are classified based\n",
    "on “Fant_arts”into “Arts” with a probability 87%,if equal 0 the\n",
    "instances are classified into “Academics” with a probability of 74%\n",
    ",else the instances are classified into “Arts” with a high probability\n",
    "96%, if “Career_sprt” is 1, the tree further branches based on the\n",
    "“Fant_arts” into “Sports” with a probability 79%. If “Fant_arts” is 1 ,\n",
    "there is another split based on the “Time_art” into “Arts” with a\n",
    "probability 48%. If “Time_art” is greater than or equal to 3, the\n",
    "instances are classified into “Arts” with a probability of 84% . On the\n",
    "other hand, if “Time_art” is less than 3, the instances are classified\n",
    "based on”Act_sprt ” into “Sports” with a probability of 58% .if\n",
    "“Act_sprt” equal 0 the instances are classified into “Academics” with a\n",
    "probability of 59%, other hand the instances are classified into\n",
    "“Sports” with a probability of 78%. If “Fant_arts” not equal 1 , the\n",
    "instances are classified into “Sports” with a high probability 94%, as\n",
    "indicated by the leaf node.\n",
    "\n",
    "**Third Confusion matrix**\n",
    "\n",
    "``` {r}\n",
    "\n",
    "# Predict on the test data\n",
    "testPred <- predict(Hobby_Data_ctree, newdata = testData, type = 'class')\n",
    "\n",
    "# Check the accuracy of the model\n",
    "accuracy <- sum(testPred == testData$`Predicted Hobby`) / nrow(testData) * 100\n",
    "cat('Accuracy:', accuracy, '\\n')\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix <- table(Actual = testData$`Predicted Hobby`, Predicted = testPred)\n",
    "\n",
    "# Calculate precision for each class\n",
    "precision_class_1 <- conf_matrix[1, 1] / sum(conf_matrix[1, ])\n",
    "precision_class_2 <- conf_matrix[2, 2] / sum(conf_matrix[2, ])\n",
    "precision_class_3 <- conf_matrix[3, 3] / sum(conf_matrix[3, ])\n",
    "\n",
    "# Calculate sensitivity for each class\n",
    "sensitivity_class_1 <- conf_matrix[1, 1] / sum(conf_matrix[1, ])\n",
    "sensitivity_class_2 <- conf_matrix[2, 2] / sum(conf_matrix[2, ])\n",
    "sensitivity_class_3 <- conf_matrix[3, 3] / sum(conf_matrix[3, ])\n",
    "\n",
    "# Calculate specificity for each class\n",
    "specificity_class_1 <- sum(diag(conf_matrix[-1, -1])) / sum(conf_matrix[-1, ])\n",
    "specificity_class_2 <- sum(conf_matrix[c(1, 3), c(1, 3)]) / sum(conf_matrix[c(1, 3), ])\n",
    "specificity_class_3 <- sum(conf_matrix[c(1, 2), c(1, 2)]) / sum(conf_matrix[c(1, 2), ])\n",
    "\n",
    "# Calculate macro-average sensitivity\n",
    "macro_avg_sensitivity <- (sensitivity_class_1 + sensitivity_class_2 + sensitivity_class_3) / 3\n",
    "\n",
    "# Calculate macro-average specificity\n",
    "macro_avg_specificity <- (specificity_class_1 + specificity_class_2 + specificity_class_3) / 3\n",
    "\n",
    "# Calculate macro-average precision\n",
    "macro_avg_precision <- (precision_class_1 + precision_class_2 + precision_class_3) / 3\n",
    "\n",
    "# Print macro-average sensitivity\n",
    "cat('Average Sensitivity:', macro_avg_sensitivity, '\\n')\n",
    "\n",
    "# Print macro-average specificity\n",
    "cat('Average Specificity:', macro_avg_specificity, '\\n')\n",
    "\n",
    "# Print macro-average precision\n",
    "cat('Average Precision:', macro_avg_precision, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print precision for each class\n",
    "cat('Precision for Class 1:', precision_class_1, ' \\n')\n",
    "\n",
    "cat('Precision for Class 2:', precision_class_2, '  \\n')\n",
    "cat('Precision for Class 3:', precision_class_3, ' \\n')\n",
    "\n",
    "# Print sensitivity for each class\n",
    "cat('Sensitivity for Class 1:', sensitivity_class_1, '\\n')\n",
    "cat('Sensitivity for Class 2:', sensitivity_class_2, '\\n')\n",
    "cat('Sensitivity for Class 3:', sensitivity_class_3, '\\n')\n",
    "\n",
    "# Print specificity for each class\n",
    "cat('Specificity for Class 1:', specificity_class_1, '\\n')\n",
    "cat('Specificity for Class 2:', specificity_class_2, '\\n')\n",
    "cat('Specificity for Class 3:', specificity_class_3, '\\n')\n",
    "\n",
    "```\n",
    "\n",
    "**Comparing Decision Tree Results Using Infromation gain:** After\n",
    "training three trees with different sizes, employing information gain as\n",
    "the selection measure, our analysis led to consistent accuracy results\n",
    "among the trees: Tree 1 (0.8932), Tree 2 (0.9057), and Tree 3 (0.8721).\n",
    "The minor discrepancies observed in these accuracy values could be\n",
    "attributed to the variations in dataset sizes. Investigating the impact\n",
    "of different training set sizes on model performance offers valuable\n",
    "insights into the intricate relationship between data size and accuracy.\n",
    "\n",
    "In the case of Tree 2, where a larger training set was employed (80%\n",
    "training, 20% testing), the model had the opportunity to grasp more\n",
    "robust patterns and relationships within the data. However, it is\n",
    "crucial to underscore the necessity of striking a balance between the\n",
    "sizes of the training and testing sets. For Tree 3, with a relatively\n",
    "smaller testing set (90% training, 10% testing), the accuracy estimate\n",
    "might be less reliable due to the limited sample size in the testing\n",
    "set.\n",
    "\n",
    "In summary, the utilization of information gain as the selection\n",
    "measure, coupled with different training set sizes, resulted in\n",
    "comparable accuracy outcomes. Achieving an optimal balance in the sizes\n",
    "of both training and testing datasets proves essential for ensuring\n",
    "accurate and generalizable model performance.\n",
    "+——————+——————-+——————-+——————-+ \\| information gain \\| 90 %t raining\n",
    "set \\| 80 %t raining set \\| 70 %t raining set \\| \\| \\| \\| \\| \\| \\| \\|\n",
    "10% testing set: \\| 20% testing set: \\| 30% testing set: \\|\n",
    "+:================:+:=================:+:=================:+:=================:+\n",
    "\\| **Accuracy** \\| 0.872 \\| 0.905 \\| 0.893 \\|\n",
    "+——————+——————-+——————-+——————-+ \\| **precision** \\| 0.856 \\| 0.898 \\|\n",
    "0.887 \\| +——————+——————-+——————-+——————-+ \\| **sensitivity** \\| 0.856 \\|\n",
    "0.898 \\| 0.887 \\| +——————+——————-+——————-+——————-+ \\| **specificity** \\|\n",
    "0.919 \\| 0.937 \\| 0.929 \\| +——————+——————-+——————-+——————-+\n",
    "\n",
    "**Gini index**\n",
    "\n",
    "The Gini index, is a measure used in decision trees, specifically in the\n",
    "CART (Classification and Regression Trees) algorithm, to quantify how\n",
    "often a randomly chosen element would be incorrectly labeled if it was\n",
    "randomly labeled according to the distribution of labels in the subset.\n",
    "It reflects the probability of a particular variable being wrongly\n",
    "classified when it is randomly chosen.\\[5\\]\n",
    "\n",
    "**1-Gini index(80%,20%)**\n",
    "\n",
    "Install necessary libraries\n",
    "\n",
    "``` {r}\n",
    "install.packages(\"rpart\")\n",
    "install.packages(\"rpart.plot\")\n",
    "install.packages(\"caTools\")\n",
    "install.packages(\"caret\")\n",
    "```\n",
    "\n",
    "Load necessary libraries\n",
    "\n",
    "``` {r}\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "library(caTools)\n",
    "library(caret)\n",
    "```\n",
    "\n",
    "Set a seed for reproducibility\n",
    "\n",
    "``` {r}\n",
    "set.seed(123)\n",
    "```\n",
    "\n",
    "Split the dataset, 80% for training, 20% for testing\n",
    "\n",
    "``` {r}\n",
    "split <- sample.split(Hobby_Data$`Predicted Hobby`, SplitRatio = 0.80)\n",
    "```\n",
    "\n",
    "Create the training set (80% of the data)\n",
    "\n",
    "``` {r}\n",
    "training_set <- subset(Hobby_Data, split == TRUE)\n",
    "```\n",
    "\n",
    "Create the test set (20% of the data)\n",
    "\n",
    "``` {r}\n",
    "test_set <- subset(Hobby_Data, split == FALSE)\n",
    "```\n",
    "\n",
    "Build a decision tree model on the training set\n",
    "\n",
    "``` {r}\n",
    "tree <- rpart(`Predicted Hobby` ~ ., data = training_set, method = 'class')\n",
    "```\n",
    "\n",
    "Make predictions on the test set using the tree model\n",
    "\n",
    "``` {r}\n",
    "predictions <- predict(tree, test_set, type = \"class\")\n",
    "```\n",
    "\n",
    "Confusion matrix\n",
    "\n",
    "``` {r}\n",
    "conf_matrix <- table(Predicted = predictions, Actual = test_set$`Predicted Hobby`)\n",
    "```\n",
    "\n",
    "Calculate accuracy\n",
    "\n",
    "``` {r}\n",
    "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
    "```\n",
    "\n",
    "Initialize vectors to hold the metrics for each class\n",
    "\n",
    "``` {r}\n",
    "precision <- numeric(length = nrow(conf_matrix))\n",
    "recall <- numeric(length = nrow(conf_matrix))\n",
    "specificity <- numeric(length = nrow(conf_matrix))\n",
    "```\n",
    "\n",
    "Calculate metrics for each class\n",
    "\n",
    "``` {r}\n",
    "for (i in 1:nrow(conf_matrix)) {\n",
    "  TP <- conf_matrix[i, i]\n",
    "  FP <- sum(conf_matrix[, i]) - TP\n",
    "  FN <- sum(conf_matrix[i, ]) - TP\n",
    "  TN <- sum(conf_matrix) - TP - FP - FN\n",
    "  \n",
    "  precision[i] <- TP / (TP + FP)\n",
    "  recall[i] <- TP / (TP + FN)\n",
    "  specificity[i] <- TN / (TN + FP)\n",
    "}\n",
    "```\n",
    "\n",
    "Average the metrics if you want a single performance measure\n",
    "\n",
    "``` {r}\n",
    "avg_precision <- mean(precision)\n",
    "avg_recall <- mean(recall)\n",
    "avg_specificity <- mean(specificity)\n",
    "```\n",
    "\n",
    "Output the evaluation metrics\n",
    "\n",
    "``` {r}\n",
    "print(paste(\"Overall Accuracy:\", accuracy))\n",
    "print(paste(\"Average Precision:\", avg_precision))\n",
    "print(paste(\"Average Recall (Sensitivity):\", avg_recall))\n",
    "print(paste(\"Average Specificity:\", avg_specificity))\n",
    "```\n",
    "\n",
    "the metrics for each class:\n",
    "\n",
    "``` {r}\n",
    "metrics <- data.frame(\n",
    "  Class = rownames(conf_matrix),\n",
    "  Precision = precision,\n",
    "  Recall = recall,\n",
    "  Specificity = specificity\n",
    ")\n",
    "```\n",
    "\n",
    "Print metrics\n",
    "\n",
    "``` {r}\n",
    "print(metrics)\n",
    "```\n",
    "\n",
    "Plot the decision tree\n",
    "\n",
    "``` {r}\n",
    "rpart.plot(tree)\n",
    "```\n",
    "\n",
    "**Decision Tree Analysis Using Gini Index(80/20):** The decision tree\n",
    "delineates hobbies into ‘Academics’ (1), ‘Arts’ (2), and ‘Sports’ (3).\n",
    "Without a sports hobby (‘Career_sprt’ = 0), the model suggests a 62%\n",
    "chance of ‘Academics’. With no arts hobby (‘Fant_arts’ = 0) and\n",
    "‘Won_arts’ at 0 or 2, there’s a 43% chance of an ‘Academics’\n",
    "categorization. Conversely, for those with an arts hobby (‘Fant_arts’\n",
    "= 1) and frequent arts activities (‘Time_art’ ≥ 3), These model show how\n",
    "likely the model is to predict each hobby based on the attributes’\n",
    "significance, as learned from the data with a 80% training portion\n",
    "\n",
    "**2-Gini index(90%,10%)**\n",
    "\n",
    "Install necessary libraries\n",
    "\n",
    "``` {r}\n",
    "install.packages(\"rpart\")\n",
    "install.packages(\"rpart.plot\")\n",
    "install.packages(\"caTools\")\n",
    "install.packages(\"caret\")\n",
    "```\n",
    "\n",
    "Load necessary libraries\n",
    "\n",
    "``` {r}\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "library(caTools)\n",
    "library(caret)\n",
    "```\n",
    "\n",
    "Set a seed for reproducibility\n",
    "\n",
    "``` {r}\n",
    "set.seed(123)\n",
    "```\n",
    "\n",
    "Split the dataset, 90% for training, 10% for testing\n",
    "\n",
    "``` {r}\n",
    "split <- sample.split(Hobby_Data$`Predicted Hobby`, SplitRatio = 0.90)\n",
    "```\n",
    "\n",
    "Create the training set (90% of the data)\n",
    "\n",
    "``` {r}\n",
    "training_set <- subset(Hobby_Data, split == TRUE)\n",
    "```\n",
    "\n",
    "Create the test set (10% of the data)\n",
    "\n",
    "``` {r}\n",
    "test_set <- subset(Hobby_Data, split == FALSE)\n",
    "```\n",
    "\n",
    "Build a decision tree model on the training set\n",
    "\n",
    "``` {r}\n",
    "tree <- rpart(`Predicted Hobby` ~ ., data = training_set, method = 'class')\n",
    "```\n",
    "\n",
    "Make predictions on the test set using the tree model\n",
    "\n",
    "``` {r}\n",
    "predictions <- predict(tree, test_set, type = \"class\")\n",
    "```\n",
    "\n",
    "Confusion matrix\n",
    "\n",
    "``` {r}\n",
    "conf_matrix <- table(Predicted = predictions, Actual = test_set$`Predicted Hobby`)\n",
    "```\n",
    "\n",
    "Calculate accuracy\n",
    "\n",
    "``` {r}\n",
    "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
    "```\n",
    "\n",
    "Initialize vectors to hold the metrics for each class\n",
    "\n",
    "``` {r}\n",
    "precision <- numeric(length = nrow(conf_matrix))\n",
    "recall <- numeric(length = nrow(conf_matrix))\n",
    "specificity <- numeric(length = nrow(conf_matrix))\n",
    "```\n",
    "\n",
    "Calculate metrics for each class\n",
    "\n",
    "``` {r}\n",
    "for (i in 1:nrow(conf_matrix)) {\n",
    "  TP <- conf_matrix[i, i]\n",
    "  FP <- sum(conf_matrix[, i]) - TP\n",
    "  FN <- sum(conf_matrix[i, ]) - TP\n",
    "  TN <- sum(conf_matrix) - TP - FP - FN\n",
    "  \n",
    "  precision[i] <- TP / (TP + FP)\n",
    "  recall[i] <- TP / (TP + FN)\n",
    "  specificity[i] <- TN / (TN + FP)\n",
    "}\n",
    "```\n",
    "\n",
    "Average the metrics if you want a single performance measure\n",
    "\n",
    "``` {r}\n",
    "avg_precision <- mean(precision)\n",
    "avg_recall <- mean(recall)\n",
    "avg_specificity <- mean(specificity)\n",
    "```\n",
    "\n",
    "Output the evaluation metrics\n",
    "\n",
    "``` {r}\n",
    "print(paste(\"Overall Accuracy:\", accuracy))\n",
    "print(paste(\"Average Precision:\", avg_precision))\n",
    "print(paste(\"Average Recall (Sensitivity):\", avg_recall))\n",
    "print(paste(\"Average Specificity:\", avg_specificity))\n",
    "```\n",
    "\n",
    "the metrics for each class:\n",
    "\n",
    "``` {r}\n",
    "metrics <- data.frame(\n",
    "  Class = rownames(conf_matrix),\n",
    "  Precision = precision,\n",
    "  Recall = recall,\n",
    "  Specificity = specificity\n",
    ")\n",
    "```\n",
    "\n",
    "Print metrics\n",
    "\n",
    "``` {r}\n",
    "print(metrics)\n",
    "```\n",
    "\n",
    "Plot the decision tree\n",
    "\n",
    "``` {r}\n",
    "rpart.plot(tree)\n",
    "```\n",
    "\n",
    "**Decision Tree Analysis Using Gini Index(90/10):**\n",
    "\n",
    "The decision tree classifies hobbies into ‘Academics’ (1), ‘Arts’ (2),\n",
    "and ‘Sports’ (3). A lack of a sports hobby (‘Career_sprt’ = 0) leads to\n",
    "a 63% chance of falling into ‘Academics’. If someone is not engaged in\n",
    "an arts hobby (‘Fant_arts’ = 0) and ‘Won_arts’ is 0 or 2, there’s a 43%\n",
    "probability of an ‘Academics’ categorization. For individuals engaged in\n",
    "an arts hobby (‘Fant_arts’ = 1) with a high level of arts activity\n",
    "(‘Time_art’ ≥ 3), the likelihood of a ‘Sports’ classification is 28%.\n",
    "These model show how likely the model is to predict each hobby based on\n",
    "the attributes’ significance, as learned from the data with a 90%\n",
    "training portion.\n",
    "\n",
    "**3-Gini index(70%,30%)**\n",
    "\n",
    "Install necessary libraries\n",
    "\n",
    "``` {r}\n",
    "install.packages(\"rpart\")\n",
    "install.packages(\"rpart.plot\")\n",
    "install.packages(\"caTools\")\n",
    "install.packages(\"caret\")\n",
    "```\n",
    "\n",
    "Load necessary libraries\n",
    "\n",
    "``` {r}\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "library(caTools)\n",
    "library(caret)\n",
    "```\n",
    "\n",
    "Set a seed for reproducibility\n",
    "\n",
    "``` {r}\n",
    "set.seed(123)\n",
    "```\n",
    "\n",
    "Split the dataset, 70% for training, 30% for testing\n",
    "\n",
    "``` {r}\n",
    "split <- sample.split(Hobby_Data$`Predicted Hobby`, SplitRatio = 0.70)\n",
    "```\n",
    "\n",
    "Create the training set (70% of the data)\n",
    "\n",
    "``` {r}\n",
    "training_set <- subset(Hobby_Data, split == TRUE)\n",
    "```\n",
    "\n",
    "Create the test set (20% of the data)\n",
    "\n",
    "``` {r}\n",
    "test_set <- subset(Hobby_Data, split == FALSE)\n",
    "```\n",
    "\n",
    "Build a decision tree model on the training set\n",
    "\n",
    "``` {r}\n",
    "tree <- rpart(`Predicted Hobby` ~ ., data = training_set, method = 'class')\n",
    "```\n",
    "\n",
    "Make predictions on the test set using the tree model\n",
    "\n",
    "``` {r}\n",
    "predictions <- predict(tree, test_set, type = \"class\")\n",
    "```\n",
    "\n",
    "Confusion matrix\n",
    "\n",
    "``` {r}\n",
    "conf_matrix <- table(Predicted = predictions, Actual = test_set$`Predicted Hobby`)\n",
    "```\n",
    "\n",
    "Calculate accuracy\n",
    "\n",
    "``` {r}\n",
    "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
    "```\n",
    "\n",
    "Initialize vectors to hold the metrics for each class\n",
    "\n",
    "``` {r}\n",
    "precision <- numeric(length = nrow(conf_matrix))\n",
    "recall <- numeric(length = nrow(conf_matrix))\n",
    "specificity <- numeric(length = nrow(conf_matrix))\n",
    "```\n",
    "\n",
    "Calculate metrics for each class\n",
    "\n",
    "``` {r}\n",
    "for (i in 1:nrow(conf_matrix)) {\n",
    "  TP <- conf_matrix[i, i]\n",
    "  FP <- sum(conf_matrix[, i]) - TP\n",
    "  FN <- sum(conf_matrix[i, ]) - TP\n",
    "  TN <- sum(conf_matrix) - TP - FP - FN\n",
    "  \n",
    "  precision[i] <- TP / (TP + FP)\n",
    "  recall[i] <- TP / (TP + FN)\n",
    "  specificity[i] <- TN / (TN + FP)\n",
    "}\n",
    "```\n",
    "\n",
    "Average the metrics if you want a single performance measure\n",
    "\n",
    "``` {r}\n",
    "avg_precision <- mean(precision)\n",
    "avg_recall <- mean(recall)\n",
    "avg_specificity <- mean(specificity)\n",
    "```\n",
    "\n",
    "Output the evaluation metrics\n",
    "\n",
    "``` {r}\n",
    "print(paste(\"Overall Accuracy:\", accuracy))\n",
    "print(paste(\"Average Precision:\", avg_precision))\n",
    "print(paste(\"Average Recall (Sensitivity):\", avg_recall))\n",
    "print(paste(\"Average Specificity:\", avg_specificity))\n",
    "```\n",
    "\n",
    "the metrics for each class:\n",
    "\n",
    "``` {r}\n",
    "metrics <- data.frame(\n",
    "  Class = rownames(conf_matrix),\n",
    "  Precision = precision,\n",
    "  Recall = recall,\n",
    "  Specificity = specificity\n",
    ")\n",
    "```\n",
    "\n",
    "Print metrics\n",
    "\n",
    "``` {r}\n",
    "print(metrics)\n",
    "```\n",
    "\n",
    "Plot the decision tree\n",
    "\n",
    "``` {r}\n",
    "rpart.plot(tree)\n",
    "```\n",
    "\n",
    "**Decision Tree Analysis Using Gini Index(70/30):**\n",
    "\n",
    "The decision tree sorts hobbies into ‘Academics’ (1), ‘Arts’ (2), and\n",
    "‘Sports’ (3). A non-sports hobby (‘Career_sprt’ = 0) results in a 63%\n",
    "probability of an ‘Academics’ categorization. If ‘Fant_arts’ is 0 and\n",
    "‘Won_arts’ is 0 or 2, there’s a 43% chance of being classified as\n",
    "‘Academics’. Conversely, for those involved in an arts hobby\n",
    "(‘Fant_arts’ = 1) with significant arts activity (‘Time_art’ ≥ 3), the\n",
    "model indicates a 28% probability of a ‘Sports’ hobby. This decision\n",
    "tree demonstrates the likelihood of predicting each hobby based on the\n",
    "importance of the attributes, as determined from the data trained with a\n",
    "70% portion.\n",
    "\n",
    "**Comparing Decision Tree Results Using Gini Index:**\n",
    "\n",
    "Across Three Training-Test Sizes: The results of the decision trees from\n",
    "the 90/10, 80/20, and 70/30 dataset splits, there is a consistent\n",
    "pattern: ‘Career_sprt’ is always the root node, and the subsequent\n",
    "splits on ‘Won_arts’ and ‘Fant_arts’ are the same across all trees. This\n",
    "consistency in tree structure and the probabilities for predicting\n",
    "‘Academics’ and ‘Sports’ across different splits suggest a stable and\n",
    "robust model that is reliable regardless of the training set size.\n",
    "\n",
    "the accuracies of three data splits reveals distinct outcomes: the\n",
    "(90,10) split leads with the highest accuracy at 0.91875, suggesting\n",
    "that a larger training portion is more effective in this case. The\n",
    "(70,30) split follows with an accuracy of 0.91060, showing strong\n",
    "performance even with a larger test set. However, the commonly used\n",
    "(80,20) split lags slightly behind, achieving an accuracy of 0.90625.\n",
    "This comparison highlights the impact of varying training and testing\n",
    "proportions on model accuracy.\n",
    "\n",
    "|   Gini index    | 90 %t raining set 10% testing set: | 80 %t raining set 20% testing set: | 70 %t raining set 30% testing set: |\n",
    "|:---------:|:------------------:|:------------------:|:------------------:|\n",
    "|  **Accuracy**   |              0.91875               |               0.906                |               0.911                |\n",
    "|  **precision**  |              0.91850               |               0.905                |               0.909                |\n",
    "| **sensitivity** |              0.91927               |               0.909                |               0.912                |\n",
    "| **specificity** |               0.9578               |               0.952                |               0.954                |\n",
    "\n",
    "**Gain Ratio** The third criterion employed for building the decision\n",
    "tree is Gain Ratio. Gain Ratio stands out as a significant metric in\n",
    "decision tree algorithms, especially in scenarios involving categorical\n",
    "target variables. It normalizes the reduction in entropy by taking into\n",
    "account the potential information content of the feature. This\n",
    "normalization process makes Gain Ratio particularly suitable for\n",
    "datasets with categorical target variables. By factoring in the\n",
    "intrinsic information of a split, Gain Ratio effectively mitigates bias\n",
    "towards features with higher levels, ensuring a more balanced evaluation\n",
    "of different attributes.\\[6\\]\n",
    "\n",
    "**1-Gain ratio(90%,10%)**\n",
    "\n",
    "Install necessary libraries\n",
    "\n",
    "``` {r}\n",
    "install.packages(\"C50\")\n",
    "install.packages(\"printr\")\n",
    "install.packages(\"caret\")\n",
    "```\n",
    "\n",
    "Load necessary libraries\n",
    "\n",
    "``` {r}\n",
    "library(C50)\n",
    "library(printr)\n",
    "library(caret)\n",
    "```\n",
    "\n",
    "Set a seed for reproducibility\n",
    "\n",
    "``` {r}\n",
    "set.seed(1958)\n",
    "```\n",
    "\n",
    "Splitting the data into training and test sets\n",
    "\n",
    "``` {r}\n",
    "train_indices <- sample(1:nrow(Hobby_Data), 0.9 * nrow(Hobby_Data))\n",
    "Hobby.train <- Hobby_Data[train_indices, ]\n",
    "Hobby.test <- Hobby_Data[-train_indices, ]\n",
    "```\n",
    "\n",
    "Training the decision tree model\n",
    "\n",
    "``` {r}\n",
    "model <- C5.0(`Predicted Hobby` ~ ., data = Hobby.train, control = C5.0Control(CF = 0.01))\n",
    "```\n",
    "\n",
    "Making predictions on the test set\n",
    "\n",
    "``` {r}\n",
    "predictions <- predict(model, newdata = Hobby.test, type = 'class')\n",
    "```\n",
    "\n",
    "Create a confusion matrix from the predictions and actual values\n",
    "\n",
    "``` {r}\n",
    "conf_matrix <- table(Predicted = predictions, Actual = Hobby.test$`Predicted Hobby`)\n",
    "```\n",
    "\n",
    "Calculate and print the accuracy of the model\n",
    "\n",
    "``` {r}\n",
    "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
    "print(paste('Accuracy on test data is:', accuracy))\n",
    "```\n",
    "\n",
    "Initialize vectors to hold the metrics for each class\n",
    "\n",
    "``` {r}\n",
    "precision <- numeric(length = nrow(conf_matrix))\n",
    "recall <- numeric(length = nrow(conf_matrix))\n",
    "specificity <- numeric(length = nrow(conf_matrix))\n",
    "```\n",
    "\n",
    "Calculate metrics for each class\n",
    "\n",
    "``` {r}\n",
    "for (i in 1:nrow(conf_matrix)) {\n",
    "  TP <- conf_matrix[i, i]\n",
    "  FP <- sum(conf_matrix[, i]) - TP\n",
    "  FN <- sum(conf_matrix[i, ]) - TP\n",
    "  TN <- sum(conf_matrix) - TP - FP - FN\n",
    "  \n",
    "  precision[i] <- TP / (TP + FP)\n",
    "  recall[i] <- TP / (TP + FN)\n",
    "  specificity[i] <- TN / (TN + FP)\n",
    "}\n",
    "```\n",
    "\n",
    "Average the metrics if you want a single performance measure\n",
    "\n",
    "``` {r}\n",
    "avg_precision <- mean(precision)\n",
    "avg_recall <- mean(recall)\n",
    "avg_specificity <- mean(specificity)\n",
    "```\n",
    "\n",
    "Output the evaluation metrics\n",
    "\n",
    "``` {r}\n",
    "print(paste(\"Overall Accuracy:\", accuracy))\n",
    "print(paste(\"Average Precision:\", avg_precision))\n",
    "print(paste(\"Average Recall (Sensitivity):\", avg_recall))\n",
    "print(paste(\"Average Specificity:\", avg_specificity))\n",
    "```\n",
    "\n",
    "print the metrics for each class:\n",
    "\n",
    "``` {r}\n",
    "\n",
    "metrics <- data.frame(\n",
    "  Class = rownames(conf_matrix),\n",
    "  Precision = precision,\n",
    "  Recall = recall,\n",
    "  Specificity = specificity\n",
    ")\n",
    "```\n",
    "\n",
    "Print metrics\n",
    "\n",
    "``` {r}\n",
    "print(metrics)\n",
    "```\n",
    "\n",
    "Generate and print additional performance metrics using caret package\n",
    "\n",
    "``` {r}\n",
    "confusionMatrix(predictions, Hobby.test$`Predicted Hobby`)\n",
    "```\n",
    "\n",
    "Plot the decision tree\n",
    "\n",
    "``` {r}\n",
    "plot(model)\n",
    "```\n",
    "\n",
    "**Decision Tree Analysis Using Gain Ratio(90%/10%):**\n",
    "\n",
    "In First Tree ,we devide dataset into training set and test set with\n",
    "size(%90,%10) respectively. As you can see in the figure, the root node\n",
    "is “Career_sprt” , class 1(“Academics”), class 2(“Arts”), and class 3\n",
    "(“Sports”).\n",
    "\n",
    "Node “Career_sprt” The first decision is based on whether the value of\n",
    "the “Career_sprt” attribute is 0. then check if “Won_arts” is either 0\n",
    "or 2.If” Won_arts” is 0 or 2 , predict”Academic”.then check If\n",
    "“Fant_arts” is 1 “and Won_arts” is 1, predict “Arts”.If “Fant_arts” is 0\n",
    "and Won_arts is 0 or 2, then check if Olympiad_Participation is 1.If\n",
    "Olympiad_Participation is 1 predict “Academics”.(When\n",
    "Olympiad_Participation is 0) If “Olympiad_Participation” is 0 and\n",
    "“Fant_arts” is 0, then check if “Grasp_pow” is less than or equal to 4 ,\n",
    "predict class”Arts”.When Career_sprt is 1, then check if “Fant_arts” is\n",
    "0 then predict “Sports”. If “Fant_arts” is 1, then check if “Time_art”\n",
    "is greater than 2.check if “Time_art” is less than or equal to 2, then\n",
    "check if “Act_sprt” is 1 or 0.If Act_sprt is 1, predict “Sports”. If\n",
    "Act_sprt is 0, then check if Olympiad_Participation is 0 predict “Arts”.\n",
    "If “Olympiad_Participation” is 1, predict “Academics”.When Time_art is\n",
    "greater than 2 ,If Won_arts is 0, predict class “Sports”.\n",
    "\n",
    "**2-Gain ratio(80%,20%)**\n",
    "\n",
    "Install necessary libraries\n",
    "\n",
    "``` {r}\n",
    "install.packages(\"C50\")\n",
    "install.packages(\"printr\")\n",
    "install.packages(\"caret\")\n",
    "```\n",
    "\n",
    "Load necessary libraries\n",
    "\n",
    "``` {r}\n",
    "\n",
    "library(C50)\n",
    "library(printr)\n",
    "library(caret)\n",
    "```\n",
    "\n",
    "Set a seed for reproducibility\n",
    "\n",
    "``` {r}\n",
    "set.seed(1958)\n",
    "```\n",
    "\n",
    "Splitting the data into training and test sets\n",
    "\n",
    "``` {r}\n",
    "train_indices <- sample(1:nrow(Hobby_Data), 0.8 * nrow(Hobby_Data))\n",
    "Hobby.train <- Hobby_Data[train_indices, ]\n",
    "Hobby.test <- Hobby_Data[-train_indices, ]\n",
    "```\n",
    "\n",
    "Training the decision tree model\n",
    "\n",
    "``` {r}\n",
    "model <- C5.0(`Predicted Hobby` ~ ., data = Hobby.train, control = C5.0Control(CF = 0.01))\n",
    "```\n",
    "\n",
    "Making predictions on the test set\n",
    "\n",
    "``` {r}\n",
    "predictions <- predict(model, newdata = Hobby.test, type = 'class')\n",
    "```\n",
    "\n",
    "Create a confusion matrix from the predictions and actual values\n",
    "\n",
    "``` {r}\n",
    "conf_matrix <- table(Predicted = predictions, Actual = Hobby.test$`Predicted Hobby`)\n",
    "```\n",
    "\n",
    "Calculate and print the accuracy of the model\n",
    "\n",
    "``` {r}\n",
    "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
    "print(paste('Accuracy on test data is:', accuracy))\n",
    "```\n",
    "\n",
    "Initialize vectors to hold the metrics for each class\n",
    "\n",
    "``` {r}\n",
    "precision <- numeric(length = nrow(conf_matrix))\n",
    "recall <- numeric(length = nrow(conf_matrix))\n",
    "specificity <- numeric(length = nrow(conf_matrix))\n",
    "```\n",
    "\n",
    "Calculate metrics for each class\n",
    "\n",
    "``` {r}\n",
    "for (i in 1:nrow(conf_matrix)) {\n",
    "  TP <- conf_matrix[i, i]\n",
    "  FP <- sum(conf_matrix[, i]) - TP\n",
    "  FN <- sum(conf_matrix[i, ]) - TP\n",
    "  TN <- sum(conf_matrix) - TP - FP - FN\n",
    "  \n",
    "  precision[i] <- TP / (TP + FP)\n",
    "  recall[i] <- TP / (TP + FN)\n",
    "  specificity[i] <- TN / (TN + FP)\n",
    "}\n",
    "```\n",
    "\n",
    "Average the metrics if you want a single performance measure\n",
    "\n",
    "``` {r}\n",
    "avg_precision <- mean(precision)\n",
    "avg_recall <- mean(recall)\n",
    "avg_specificity <- mean(specificity)\n",
    "```\n",
    "\n",
    "Output the evaluation metrics\n",
    "\n",
    "``` {r}\n",
    "print(paste(\"Overall Accuracy:\", accuracy))\n",
    "print(paste(\"Average Precision:\", avg_precision))\n",
    "print(paste(\"Average Recall (Sensitivity):\", avg_recall))\n",
    "print(paste(\"Average Specificity:\", avg_specificity))\n",
    "```\n",
    "\n",
    "print the metrics for each class:\n",
    "\n",
    "``` {r}\n",
    "\n",
    "metrics <- data.frame(\n",
    "  Class = rownames(conf_matrix),\n",
    "  Precision = precision,\n",
    "  Recall = recall,\n",
    "  Specificity = specificity\n",
    ")\n",
    "```\n",
    "\n",
    "Print metrics\n",
    "\n",
    "``` {r}\n",
    "print(metrics)\n",
    "```\n",
    "\n",
    "Generate and print additional performance metrics using caret package\n",
    "\n",
    "``` {r}\n",
    "confusionMatrix(predictions, Hobby.test$`Predicted Hobby`)\n",
    "```\n",
    "\n",
    "Plot the decision tree\n",
    "\n",
    "``` {r}\n",
    "plot(model)\n",
    "```\n",
    "\n",
    "**Decision Tree Analysis Using Gain Ratio(80/20):**\n",
    "\n",
    "The decision tree depicted classifies hobbies into ‘Academics’ (1),\n",
    "‘Arts’ (2), and ‘Sports’ (3). It starts with ‘Career_sprt’ a value of 0\n",
    "leads to ‘Won_arts’. If ‘Won_arts’ is 0 or 2, the model suggests\n",
    "‘Academics’ or ‘Arts’. If ‘Career_sprt’ is 1, ‘Fant_arts’ is considered\n",
    "next; a value of 0 after ‘Won_arts’ being 1 points towards ‘Arts’, while\n",
    "a value of 1 leads to ‘Olympiad_Participation’, which, if 1, indicates\n",
    "‘Academics’. Conversely, a high ‘Grasp_pow’ (\\>4) predicts ‘Sports’.\n",
    "\n",
    "**3-Gain ratio(70%,30%)**\n",
    "\n",
    "Install necessary libraries\n",
    "\n",
    "``` {r}\n",
    "install.packages(\"C50\")\n",
    "install.packages(\"printr\")\n",
    "install.packages(\"caret\")\n",
    "```\n",
    "\n",
    "Load necessary libraries\n",
    "\n",
    "``` {r}\n",
    "\n",
    "library(C50)\n",
    "library(printr)\n",
    "library(caret)\n",
    "```\n",
    "\n",
    "Set a seed for reproducibility\n",
    "\n",
    "``` {r}\n",
    "set.seed(1958)\n",
    "```\n",
    "\n",
    "Splitting the data into training and test sets\n",
    "\n",
    "``` {r}\n",
    "train_indices <- sample(1:nrow(Hobby_Data), 0.7 * nrow(Hobby_Data))\n",
    "Hobby.train <- Hobby_Data[train_indices, ]\n",
    "Hobby.test <- Hobby_Data[-train_indices, ]\n",
    "```\n",
    "\n",
    "Training the decision tree model\n",
    "\n",
    "``` {r}\n",
    "model <- C5.0(`Predicted Hobby` ~ ., data = Hobby.train, control = C5.0Control(CF = 0.01))\n",
    "```\n",
    "\n",
    "Making predictions on the test set\n",
    "\n",
    "``` {r}\n",
    "predictions <- predict(model, newdata = Hobby.test, type = 'class')\n",
    "```\n",
    "\n",
    "Create a confusion matrix from the predictions and actual values\n",
    "\n",
    "``` {r}\n",
    "conf_matrix <- table(Predicted = predictions, Actual = Hobby.test$`Predicted Hobby`)\n",
    "```\n",
    "\n",
    "Calculate and print the accuracy of the model\n",
    "\n",
    "``` {r}\n",
    "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
    "print(paste('Accuracy on test data is:', accuracy))\n",
    "```\n",
    "\n",
    "Initialize vectors to hold the metrics for each class\n",
    "\n",
    "``` {r}\n",
    "precision <- numeric(length = nrow(conf_matrix))\n",
    "recall <- numeric(length = nrow(conf_matrix))\n",
    "specificity <- numeric(length = nrow(conf_matrix))\n",
    "```\n",
    "\n",
    "Calculate metrics for each class\n",
    "\n",
    "``` {r}\n",
    "for (i in 1:nrow(conf_matrix)) {\n",
    "  TP <- conf_matrix[i, i]\n",
    "  FP <- sum(conf_matrix[, i]) - TP\n",
    "  FN <- sum(conf_matrix[i, ]) - TP\n",
    "  TN <- sum(conf_matrix) - TP - FP - FN\n",
    "  \n",
    "  precision[i] <- TP / (TP + FP)\n",
    "  recall[i] <- TP / (TP + FN)\n",
    "  specificity[i] <- TN / (TN + FP)\n",
    "}\n",
    "```\n",
    "\n",
    "Average the metrics if you want a single performance measure\n",
    "\n",
    "``` {r}\n",
    "avg_precision <- mean(precision)\n",
    "avg_recall <- mean(recall)\n",
    "avg_specificity <- mean(specificity)\n",
    "```\n",
    "\n",
    "Output the evaluation metrics\n",
    "\n",
    "``` {r}\n",
    "print(paste(\"Overall Accuracy:\", accuracy))\n",
    "print(paste(\"Average Precision:\", avg_precision))\n",
    "print(paste(\"Average Recall (Sensitivity):\", avg_recall))\n",
    "print(paste(\"Average Specificity:\", avg_specificity))\n",
    "```\n",
    "\n",
    "print the metrics for each class:\n",
    "\n",
    "``` {r}\n",
    "\n",
    "metrics <- data.frame(\n",
    "  Class = rownames(conf_matrix),\n",
    "  Precision = precision,\n",
    "  Recall = recall,\n",
    "  Specificity = specificity\n",
    ")\n",
    "```\n",
    "\n",
    "Print metrics\n",
    "\n",
    "``` {r}\n",
    "print(metrics)\n",
    "```\n",
    "\n",
    "Generate and print additional performance metrics using caret package\n",
    "\n",
    "``` {r}\n",
    "confusionMatrix(predictions, Hobby.test$`Predicted Hobby`)\n",
    "```\n",
    "\n",
    "Plot the decision tree\n",
    "\n",
    "``` {r}\n",
    "plot(model)\n",
    "```\n",
    "\n",
    "**Decision Tree Analysis Using Gain Ratio(70%/30%):**\n",
    "\n",
    "In Third Tree ,we devide dataset into training set and test set with\n",
    "size(%70,%30) respectively. As you can see in the figure, the root node\n",
    "is “Career_sprt” , class 1(“Academics”), class 2(“Arts”), and class 3\n",
    "(“Sports”).\n",
    "\n",
    "The first decision is based on whether the value of the “Career_sprt”\n",
    "attribute is 0.If “Career_sprt” is 0, then check if “Won_arts” is either\n",
    "0 or 2.predict “Academics”.If “Won_arts” is 1, then check if “Fant_arts”\n",
    "is 1,predict “Arts”.If ““Fant_arts is 0 and”Won_arts” is 0 or 2, then\n",
    "check if “Time_art” is less than or equal to 2,predict “Academics”. If\n",
    "“Time_art” is greater than 2, predict “Arts”.\n",
    "\n",
    "If “Career_sprt”is 1, then check if “Fant_arts” is 0, predict\n",
    "“Sports”.If “Fant_arts”is 1, then check if “Time_art” is less than or\n",
    "equal to 2 check if “Act_sprt” is 0,predict “Academics” .If “Act_sprt”\n",
    "is 1 predict “Sports”. if “Time_art” is greater than 2, then check if\n",
    "“Won_arts” is 0 predict “Sports”.If “Won_arts” is 1 or 2 predict “Arts”.\n",
    "\n",
    "**Comparing Decision Tree Results Using Gain Ratio**\n",
    "\n",
    "Across Three Training-Test Sizes: The accuracy rates -0.8944 for the\n",
    "90:10 split, 0.8939 for the 70:30 split, and 0.8879 for the 80:20 split\n",
    "– indicate only slight variations, with the 90:10 split being marginally\n",
    "better.\n",
    "\n",
    "<table style=\"width:98%;\">\n",
    "<colgroup>\n",
    "<col style=\"width: 22%\" />\n",
    "<col style=\"width: 25%\" />\n",
    "<col style=\"width: 25%\" />\n",
    "<col style=\"width: 25%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th style=\"text-align: center;\">Gain ratio</th>\n",
    "<th style=\"text-align: center;\"><p>90 %t raining set</p>\n",
    "<p>10% testing set:</p></th>\n",
    "<th style=\"text-align: center;\"><p>80 %t raining set</p>\n",
    "<p>20% testing set:</p></th>\n",
    "<th style=\"text-align: center;\"><p>70 %t raining set</p>\n",
    "<p>30% testing set:</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td style=\"text-align: center;\"><strong>Accuracy</strong></td>\n",
    "<td style=\"text-align: center;\">0.8944</td>\n",
    "<td style=\"text-align: center;\">0.888</td>\n",
    "<td style=\"text-align: center;\">0.893970893970894</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td style=\"text-align: center;\"><strong>precision</strong></td>\n",
    "<td style=\"text-align: center;\">0.892</td>\n",
    "<td style=\"text-align: center;\">0.881</td>\n",
    "<td style=\"text-align: center;\">0.890</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td style=\"text-align: center;\"><strong>sensitivity</strong></td>\n",
    "<td style=\"text-align: center;\">0.891</td>\n",
    "<td style=\"text-align: center;\">0.890</td>\n",
    "<td style=\"text-align: center;\">0.902</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td style=\"text-align: center;\"><strong>specificity</strong></td>\n",
    "<td style=\"text-align: center;\">0.945</td>\n",
    "<td style=\"text-align: center;\">0.943</td>\n",
    "<td style=\"text-align: center;\">0.947</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "**#2#Clustering**\n",
    "\n",
    "first we use Hobby_proc that is version of Hobby data after\n",
    "pre-processing\n",
    "\n",
    "``` {r}\n",
    "str(Hobby_proc)\n",
    "```\n",
    "\n",
    "**To convert type from factor columns to numeric**\n",
    "\n",
    "``` {r}\n",
    "\n",
    "Hobby_proc$Fav_sub <-as.numeric(Hobby_proc$Fav_sub)\n",
    "\n",
    "Hobby_proc$Olympiad_Participation <-as.numeric(Hobby_proc$Olympiad_Participation)\n",
    "\n",
    "Hobby_proc$Projects <-as.numeric(Hobby_proc$Projects)\n",
    "\n",
    "Hobby_proc$Scholarship <-as.numeric(Hobby_proc$Scholarship)\n",
    "\n",
    "Hobby_proc$Career_sprt <-as.numeric(Hobby_proc$Career_sprt)\n",
    "\n",
    "Hobby_proc$Act_sprt <-as.numeric(Hobby_proc$Act_sprt)\n",
    "\n",
    "Hobby_proc$Fant_arts <-as.numeric(Hobby_proc$Fant_arts)\n",
    "\n",
    "Hobby_proc$Won_arts <-as.numeric(Hobby_proc$Won_arts)\n",
    "\n",
    "Hobby_proc$`Predicted Hobby` <-as.numeric(Hobby_proc$`Predicted Hobby` )\n",
    "```\n",
    "\n",
    "**Scaled All Columns**\n",
    "\n",
    "``` {r}\n",
    "Hobby_proc <-scale(Hobby_proc)\n",
    "```\n",
    "\n",
    "**Data set without ground truth**\n",
    "\n",
    "``` {r}\n",
    "Hobby_Data2 <- Hobby_proc[, !(colnames(Hobby_proc) %in% c(\"Predict Hobby\"))]\n",
    "```\n",
    "\n",
    "Our data set exhibits a balanced distribution among class labels, with\n",
    "“academic,” “art,” and “sport” constituting 43.6%, 25.6%, and 30.8% of\n",
    "the total, respectively. This balanced distribution is advantageous for\n",
    "both classification and clustering tasks. In classification, a balanced\n",
    "data set helps prevent the model from favoring one class over the\n",
    "others, ensuring that the learning algorithm is exposed to a\n",
    "representative set of examples from each category. This balance promotes\n",
    "the development of a model that generalizes well across all classes,\n",
    "enhancing its predictive performance on new, unseen data. In clustering,\n",
    "a balanced data set aids in forming clusters that are more evenly\n",
    "distributed, allowing for a comprehensive understanding of patterns and\n",
    "relationships across diverse categories. Balanced data sets often lead\n",
    "to more accurate and fair clustering results, enabling meaningful\n",
    "insights into the underlying structures within each class.\n",
    "\n",
    "**require packages/Library**\n",
    "\n",
    "``` {r}\n",
    "install.packages(\"ggplot2\") \n",
    "install.packages(\"magrittr\")\n",
    "install.packages(\"dplyr\")\n",
    "library(factoextra) \n",
    "library(cluster)\n",
    "library(dplyr)\n",
    "```\n",
    "\n",
    "**k-mean clustering**\n",
    "\n",
    "**Validation:** Determining the right number of clusters before starting\n",
    "the clustering process is like making sure you have the correct-sized\n",
    "puzzle pieces before putting the puzzle together. It’s important because\n",
    "some clustering algorithms, like K-means, require such a parameter. In\n",
    "addition to that, it helps make the clustering more accurate and useful.\n",
    "If you know the right number beforehand, it saves time and helps make\n",
    "the whole process more efficient and the results more reliable.\\[8\\]\n",
    "\n",
    "**compute average silhouette for k clusters using silhouette() For\n",
    "k-mean**\n",
    "\n",
    "``` {r}\n",
    "silhouette_score <- function(k) {\n",
    "  km <- kmeans(Hobby_Data2, centers = k, nstart = 25)\n",
    "  ss <- silhouette(km$cluster, dist(Hobby_Data2))\n",
    "  sil <- mean(ss[, 3])\n",
    "  return(sil)\n",
    "}\n",
    "\n",
    "# k cluster range from 2 to 10\n",
    "k <- 2:10\n",
    "\n",
    "# call function for each k value\n",
    "avg_sil <- sapply(k, silhouette_score)\n",
    "\n",
    "# plot the results\n",
    "plot(k, avg_sil, type = 'b', xlab = 'Number of clusters', ylab = 'Average Silhouette Scores', frame = FALSE)\n",
    "```\n",
    "\n",
    "The results indicate that the highest average silhouette scores indicate\n",
    "the quality of the clusters and suggest better-defined and more\n",
    "separated clusters, with each point having a high degree of similarity\n",
    "to its own cluster and a lower similarity to neighboring clusters. The\n",
    "highest average silhouette scores were observed for k values of 3, 2,\n",
    "and 4, so these are the optimal numbers. These values will be employed\n",
    "in subsequent k-means clustering analyses.\n",
    "\n",
    "**k-means cluster k=3**\n",
    "\n",
    "``` {r}\n",
    "#set a seed for random number generation  to make the results reproducible\n",
    "set.seed(7)\n",
    "kmeans.result <- kmeans(Hobby_Data2, 3)\n",
    "\n",
    "# print the clusterng result\n",
    "kmeans.result\n",
    "\n",
    "#visualize clustering\n",
    "fviz_cluster(kmeans.result, data = Hobby_Data2)\n",
    "```\n",
    "\n",
    "**Average for each cluster**\n",
    "\n",
    "``` {r}\n",
    "avg_sil <- silhouette(kmeans.result$cluster,dist(Hobby_Data2)) \n",
    "fviz_silhouette(avg_sil)\n",
    "```\n",
    "\n",
    "the presence of negative silhouettes for some observations within each\n",
    "cluster indicates that these points might be more similar to points in\n",
    "other clusters, suggesting a potential overlap or ambiguity in their\n",
    "assignment. The fact that some observations have negative silhouettes\n",
    "highlights that the separation of data points is not entirely\n",
    "sufficient.\n",
    "\n",
    "So since 3, which was the optimal number of clusters with the highest\n",
    "silhouette score average, did not have good clustering results, that\n",
    "does support our research results that the k-means algorithm is not\n",
    "applicable to categoricaldata clustering because it relies on the\n",
    "Euclidean distance metric tomeasure the similarity between data points.\n",
    "However, even afterEncoding and its application to categorical data pose\n",
    "significant challenges. Categorical variables often lack a meaningful\n",
    "numericalrepresentation; for instance, taking the mean of categories\n",
    "like thefeature “favorite subject,” aka Fav_sub” (even after encoding),\n",
    "might nothave any practical interpretation. And the distances calculated\n",
    "in thealgorithm may not reflect the true dissimilarities between\n",
    "categoricalvalues. The encoding process itself introduces artificial\n",
    "numericalrelationships that may mislead the algorithm. Moreover, k-means\n",
    "relieson the minimization of Euclidean distances, which might not\n",
    "accuratelycapture the dissimilarity structure in categorical data.\n",
    "Categoricalvariables inherently exhibit discrete and non-ordinal\n",
    "characteristicsthat are not well-suited for the continuous and linear\n",
    "assumptions ofk-means. Alternative clustering techniques, specifically\n",
    "designed forcategorical data, such as partitioning around medoids, are\n",
    "moreappropriate for capturing the intrinsic patterns and relationships\n",
    "incategorical datasets.\\[7\\]\n",
    "\n",
    "**k-mediods clustering with PAM**\n",
    "\n",
    "K-medoids clustering presents a robust alternative for analyzing\n",
    "categorical data by addressing the limitations posed by k-means\n",
    "clustering. Unlike k-means, k-medoids does not rely on the mean as a\n",
    "representative centroid but employs medoids, which are actual data\n",
    "points within the clusters, and the algorithm defines clusters based on\n",
    "partitioning around medoids. This feature makes k-medoids particularly\n",
    "suitable for categorical data, where meaningful centroids may not have a\n",
    "numerical interpretation. The algorithm defines clusters based on\n",
    "partitioning around medoids.\n",
    "\n",
    "**Validation:**\n",
    "\n",
    "First, we want to determine three different numbers of clusters by using\n",
    "a number of methods that will suggest the optimal number of clusters for\n",
    "k-mediods clustering with PAM.\\[8\\]\n",
    "\n",
    "**Silhouette coefficient**\n",
    "\n",
    "``` {r}\n",
    "fviz_nbclust(Hobby_Data2, pam, method = \"silhouette\")+\n",
    "  labs(subtitle = \"Silhouette method\")\n",
    "```\n",
    "\n",
    "**Elbow method**\n",
    "\n",
    "``` {r}\n",
    "fviz_nbclust(Hobby_Data2, pam, method = \"wss\") +\n",
    "  geom_vline(xintercept= 3, linetype= 3)+\n",
    "  labs(subtitle = \"Elbow method\")\n",
    "```\n",
    "\n",
    "The silhouette coefficient, which measures the cohesion and separation\n",
    "of clusters, aligns with the Elbow method, which assesses the\n",
    "within-cluster sum of squares (wss) as a function of the number of\n",
    "clusters in the suggested optimal number of clusters. This alignment in\n",
    "results between two distinct evaluation methods strengthens confidence\n",
    "in the choice of three clusters, providing a stable foundation for\n",
    "further analysis and interpretation of the underlying patterns within\n",
    "the dataset. In addition to that, the ground truth (class labels) also\n",
    "contains three classes, and that indicates a reassuring alignment\n",
    "between the structure of the data and the clustering results.\n",
    "\n",
    "We need to determine two more suggested numbers of clusters by computing\n",
    "the average silhouette for k clusters using silhouette().\n",
    "\n",
    "``` {r}\n",
    "silhouette_score <- function(k) {\n",
    "  km <- pam(Hobby_Data2, k, diss = TRUE)\n",
    "  ss <- silhouette(km$clustering, dist(Hobby_Data2))\n",
    "  sil <- mean(ss[, 3])\n",
    "  return(sil)\n",
    "}\n",
    "\n",
    "# k cluster range from 2 to 10\n",
    "k <- 2:10\n",
    "\n",
    "# Call function for each k value\n",
    "avg_sil <- sapply(k, silhouette_score)\n",
    "\n",
    "# Plot the results\n",
    "plot(k, avg_sil, type = 'b', xlab = 'Number of clusters', ylab = 'Average Silhouette Scores', frame = FALSE)\n",
    "```\n",
    "\n",
    "It is a common practice to choose the number of clusters corresponding\n",
    "to the peak in the silhouette score plot, and since we are looking for\n",
    "two more number of clusters other than three, it would be reasonable to\n",
    "consider two and four clusters for further analysis. Especially with the\n",
    "decreasing trend beyond three clusters, it indicates that adding more\n",
    "clusters does not significantly improve the separation and cohesion of\n",
    "the clusters.\n",
    "\n",
    "**group into k=3 clusters**\n",
    "\n",
    "The sub-sampling and clustering approach is a helpful method to evaluate\n",
    "the robustness of the clustering results under various subsets and to\n",
    "obtain insights into the structure of the data. Furthermore, we will\n",
    "take 100 samples from our data set to ensure that the clustering plot\n",
    "doesn’t get too crowded.\n",
    "\n",
    "``` {r}\n",
    "set.seed(7)\n",
    "\n",
    "# Specify the number of rows you want to sample\n",
    "num_rows <- 100\n",
    "\n",
    "# Use sample with the specified seed\n",
    "idx <- sample(1:dim(Hobby_Data2)[1], num_rows)\n",
    "\n",
    "Hobby_Data3 <- Hobby_Data2[idx, ]\n",
    "\n",
    "pam.result <- pam(Hobby_Data3,3)\n",
    "#Show the silhoutee plot of PAM AND clusters\n",
    "plot(pam.result)\n",
    "```\n",
    "\n",
    "The output of the code, displaying a silhouette plot of the PAM\n",
    "clusters, indicates that the clusters are relatively close to each\n",
    "other, with a slight overlap between two clusters. The silhouette plot\n",
    "visually represents how well-defined and separated the clusters are.\n",
    "While the slight overlap suggests that the natural grouping within the\n",
    "dataset may not be entirely distinct, it does not negatively affect the\n",
    "overall quality of the clusters. In fact, the observed overlap might\n",
    "indicate shared characteristics between adjacent clusters, effectively\n",
    "capturing meaningful patterns and groupings that reflect the intricacies\n",
    "of real-world phenomena not confined to strict boundaries.\n",
    "\n",
    "wws\n",
    "\n",
    "``` {r}\n",
    "# Extract the clusinfo component\n",
    "clusinfo <- pam.result$clusinfo\n",
    "\n",
    "# Calculate the total within-cluster sum of squares\n",
    "tot_withinss <- sum(clusinfo[, \"size\"] * clusinfo[, \"av_diss\"]^2)\n",
    "\n",
    "# Print the result\n",
    "print(tot_withinss)\n",
    "```\n",
    "\n",
    "bCubed\n",
    "\n",
    "``` {r}\n",
    "cluster_assignments <- c(pam.result$cluster)\n",
    "set.seed(7)\n",
    "\n",
    "# Specify the number of rows you want to sample\n",
    "num_rows <- 100\n",
    "\n",
    "# Use sample with the specified seed\n",
    "idx <- sample(1:dim(Hobby_proc)[1], num_rows)\n",
    "\n",
    "# Select the sampled rows from Hobby_proc\n",
    "Hobby_Data4 <- Hobby_proc[idx, ]\n",
    "ground_truth_labels <- c(Hobby_Data4)\n",
    "\n",
    "# Create a data frame with cluster assignments and ground truth labels\n",
    "dataset <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)\n",
    "\n",
    "# Calculate BCubed precision and recall\n",
    "calculate_bcubed_metrics <- function(dataset) {\n",
    "  n <- nrow(dataset)\n",
    "  precision_sum <- 0 \n",
    "  recall_sum <- 0\n",
    "  \n",
    "   for (i in 1:n) {\n",
    "    cluster <- dataset$cluster[i] \n",
    "    label <- dataset$label[i]\n",
    "    \n",
    "    # Count the number of items from the same category in its cluster\n",
    "    same_category <- sum(dataset$label[dataset$cluster == cluster] == label)   \n",
    "    \n",
    "    # Count the number of items in its cluster    \n",
    "    same_cluster <- sum(dataset$cluster == cluster)\n",
    "    \n",
    "    # Count the number of items in its category\n",
    "    total_same_category <- sum(dataset$label == label)   \n",
    "    \n",
    "    # Calculate precision and recall \n",
    "    precision_sum <- precision_sum + same_category / same_cluster\n",
    "    recall_sum <- recall_sum + same_category / total_same_category \n",
    "    }\n",
    "  # End loop \n",
    "  \n",
    "  # Calculate average precision and recall  \n",
    "  precision <- precision_sum / n\n",
    "  recall <- recall_sum / n \n",
    "  return(list(precision = precision, recall = recall))}\n",
    "\n",
    "  # Calculate BCubed precision and recall\n",
    "  metrics <- calculate_bcubed_metrics(dataset)\n",
    "  precision <- metrics$precision\n",
    "  recall <- metrics$recall\n",
    "\n",
    "# Print the results\n",
    "  cat(\"BCubed Precision= \", precision, \"AND BCubed Recall= \", recall, \"\\n\")\n",
    "```\n",
    "\n",
    "While precision highlights room for better accuracy in identifying\n",
    "similar items, the higher recall indicates the algorithm’s capability to\n",
    "catch a good amount of actual similarities within clusters.\n",
    "\n",
    "**group into k=4 clusters**\n",
    "\n",
    "``` {r}\n",
    "set.seed(7)\n",
    "\n",
    "# Specify the number of rows you want to sample\n",
    "num_rows <- 100\n",
    "\n",
    "# Use sample with the specified seed\n",
    "idx <- sample(1:dim(Hobby_Data2)[1], num_rows)\n",
    "\n",
    "Hobby_Data3 <- Hobby_Data2[idx, ]\n",
    "\n",
    "pam.result <- pam(Hobby_Data3,4)\n",
    "#Show the silhoutee plot of PAM AND clusters\n",
    "plot(pam.result)\n",
    "```\n",
    "\n",
    "wws\n",
    "\n",
    "``` {r}\n",
    "# Extract the clusinfo component\n",
    "clusinfo <- pam.result$clusinfo\n",
    "\n",
    "# Calculate the total within-cluster sum of squares\n",
    "tot_withinss <- sum(clusinfo[, \"size\"] * clusinfo[, \"av_diss\"]^2)\n",
    "\n",
    "# Print the result\n",
    "print(tot_withinss)\n",
    "```\n",
    "\n",
    "The output suggests that the dataset may exhibit a degree of overlap or\n",
    "similarity among observations. The overlapping clusters may indicate\n",
    "challenges in achieving a clear separation among these groups. The\n",
    "placement of one cluster on top of two others implies that the medoid of\n",
    "this cluster might be near points belonging to those two neighboring\n",
    "clusters. This could be due to the nature of the data.\n",
    "\n",
    "BCubed\n",
    "\n",
    "``` {r}\n",
    "cluster_assignments <- c(pam.result$cluster) \n",
    " \n",
    "set.seed(7) \n",
    " \n",
    "# Specify the number of rows you want to sample \n",
    "num_rows <- 100 \n",
    " \n",
    "# Use sample with the specified seed \n",
    "idx <- sample(1:dim(Hobby_proc)[1], num_rows) \n",
    " \n",
    "# Select the sampled rows from Hobby_proc \n",
    "Hobby_Data4 <- Hobby_proc[idx, ] \n",
    "\n",
    "# Create a data frame with cluster assignments and ground truth labels\n",
    "dataset <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)\n",
    "\n",
    "# Calculate BCubed precision and recall\n",
    "calculate_bcubed_metrics <- function(dataset) {\n",
    "  n <- nrow(dataset)\n",
    "  precision_sum <- 0\n",
    "  recall_sum <- 0\n",
    " \n",
    "  for (i in 1:n) {\n",
    "    cluster <- dataset$cluster[i]\n",
    "    label <- dataset$label[i]\n",
    "   \n",
    "    # Count the number of items from the same category in its cluster\n",
    "    same_category <- sum(dataset$label[dataset$cluster == cluster] == label)\n",
    "   \n",
    "    # Count the number of items in its cluster\n",
    "    same_cluster <- sum(dataset$cluster == cluster)\n",
    "   \n",
    "    # Count the number of items in its category\n",
    "    total_same_category <- sum(dataset$label == label)\n",
    "   \n",
    "    # Calculate precision and recall\n",
    "    precision_sum <- precision_sum + same_category / same_cluster\n",
    "    recall_sum <- recall_sum + same_category / total_same_category\n",
    "  }\n",
    "  # End loop\n",
    " \n",
    "  # Calculate average precision and recall\n",
    "  precision <- precision_sum / n\n",
    "  recall <- recall_sum / n\n",
    " \n",
    "  return(list(precision = precision, recall = recall))\n",
    "}\n",
    "\n",
    "# Calculate BCubed precision and recall\n",
    "metrics <- calculate_bcubed_metrics(dataset)\n",
    "precision <- metrics$precision\n",
    "recall <- metrics$recall\n",
    "\n",
    "# Print the results\n",
    "cat(\"BCubed Precision= \", precision, \"AND BCubed Recall= \", recall, \"\\n\")\n",
    "```\n",
    "\n",
    "The results indicate challenges in clustering performance. The low\n",
    "precision suggests a significant rate of misclassification, while the\n",
    "relatively low recall indicates that some instances within the same\n",
    "group are missed or incorrectly assigned to other clusters. These\n",
    "results highlight limitations in accurately capturing the data’s\n",
    "underlying structure.\n",
    "\n",
    "**group into k=2 clusters**\n",
    "\n",
    "``` {r}\n",
    "set.seed(7)\n",
    "\n",
    "# Specify the number of rows you want to sample\n",
    "num_rows <- 100\n",
    "\n",
    "# Use sample with the specified seed\n",
    "idx <- sample(1:dim(Hobby_Data2)[1], num_rows)\n",
    "\n",
    "Hobby_Data3 <- Hobby_Data2[idx, ]\n",
    "\n",
    "pam.result <- pam(Hobby_Data3,2)\n",
    "#Show the silhoutee plot of PAM AND clusters\n",
    "plot(pam.result)\n",
    "```\n",
    "\n",
    "wws\n",
    "\n",
    "``` {r}\n",
    "# Extract the clusinfo component\n",
    "clusinfo <- pam.result$clusinfo\n",
    "\n",
    "# Calculate the total within-cluster sum of squares\n",
    "tot_withinss <- sum(clusinfo[, \"size\"] * clusinfo[, \"av_diss\"]^2)\n",
    "\n",
    "# Print the result\n",
    "print(tot_withinss)\n",
    "```\n",
    "\n",
    "An overlap between clusters implies that there is ambiguity in the\n",
    "assignment of data points to clusters, and the clusters may not be\n",
    "sufficiently distinct. In such cases, it might be needed to reconsider\n",
    "the number of clusters since the goal is to find a balance, but too few\n",
    "clusters result in oversimplification, as indicated by the observed\n",
    "overlap, and it is evident that forming only two clusters may not be\n",
    "sufficient to represent the inherent structure of the dataset.\n",
    "\n",
    "BCubed\n",
    "\n",
    "``` {r}\n",
    "cluster_assignments <- c(pam.result$cluster) \n",
    " \n",
    "set.seed(7) \n",
    " \n",
    "# Specify the number of rows you want to sample \n",
    "num_rows <- 100 \n",
    " \n",
    "# Use sample with the specified seed \n",
    "idx <- sample(1:dim(Hobby_proc)[1], num_rows) \n",
    " \n",
    "# Select the sampled rows from Hobby_proc \n",
    "Hobby_Data4 <- Hobby_proc[idx, ] \n",
    "\n",
    "# Create a data frame with cluster assignments and ground truth labels\n",
    "dataset <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)\n",
    "\n",
    "# Calculate BCubed precision and recall\n",
    "calculate_bcubed_metrics <- function(dataset) {\n",
    "  n <- nrow(dataset)\n",
    "  precision_sum <- 0\n",
    "  recall_sum <- 0\n",
    " \n",
    "  for (i in 1:n) {\n",
    "    cluster <- dataset$cluster[i]\n",
    "    label <- dataset$label[i]\n",
    "   \n",
    "    # Count the number of items from the same category in its cluster\n",
    "    same_category <- sum(dataset$label[dataset$cluster == cluster] == label)\n",
    "   \n",
    "    # Count the number of items in its cluster\n",
    "    same_cluster <- sum(dataset$cluster == cluster)\n",
    "   \n",
    "    # Count the number of items in its category\n",
    "    total_same_category <- sum(dataset$label == label)\n",
    "   \n",
    "    # Calculate precision and recall\n",
    "    precision_sum <- precision_sum + same_category / same_cluster\n",
    "    recall_sum <- recall_sum + same_category / total_same_category\n",
    "  }\n",
    "  # End loop\n",
    " \n",
    "  # Calculate average precision and recall\n",
    "  precision <- precision_sum / n\n",
    "  recall <- recall_sum / n\n",
    " \n",
    "  return(list(precision = precision, recall = recall))\n",
    "}\n",
    "\n",
    "# Calculate BCubed precision and recall\n",
    "metrics <- calculate_bcubed_metrics(dataset)\n",
    "precision <- metrics$precision\n",
    "recall <- metrics$recall\n",
    "\n",
    "# Print the results\n",
    "cat(\"BCubed Precision= \", precision, \"AND BCubed Recall= \", recall, \"\\n\")\n",
    "```\n",
    "\n",
    "The relatively high recall could be influenced by the specific choice of\n",
    "two clusters since it is sensitive to the number of clusters. In the\n",
    "context of a two-cluster solution, the recall score reflects how well\n",
    "the algorithm groups data points from the same class into one of the two\n",
    "identified clusters. Recall suggests that a significant portion of data\n",
    "points from the same ground truth class are indeed grouped together in\n",
    "one of the two clusters. However, it’s important to note that the low\n",
    "precision score (0.0432) indicates a lack of homogeneity within the\n",
    "identified clusters, implying that the clusters contain a mix of data\n",
    "points from different ground truth classes.\n",
    "\n",
    "**AS A Summary For clustring** Silhouette analysis measures how similar\n",
    "an object is to its own cluster (cohesion) compared to other clusters\n",
    "(separation). The silhouette width ranges from -1 to 1, where a high\n",
    "value indicates that the object is well matched to its own cluster and\n",
    "poorly matched to neighboring clusters. In our project we did clusters\n",
    "for k=3, k=4, and k=2 since its have higher average silhouette then\n",
    "other number.\n",
    "\n",
    "For k=3, the average silhouette width is 0.22. For k=4, the average\n",
    "silhouette width is 0.19. For k=2, the average silhouette width is 0.21.\n",
    "\n",
    "A higher average silhouette width generally indicates better-defined\n",
    "clusters. So, in this case, k=3 has the highest average silhouette\n",
    "width.\n",
    "\n",
    "BCubed is a clustering evaluation metric that considers both precision\n",
    "and recall. Precision measures the accuracy of the positive predictions,\n",
    "while recall measures the coverage of the actual positive instances.\n",
    "\n",
    "For k=3, BBCubed Precision = 0.0488 BCubed Recall = 0.4932. For\n",
    "k=4,BCubed Precision = 0.0505 BCubed Recall = 0.4101. For k=2,\n",
    "BCubedPrecision = 0.0432 BCubed Recall = 0.6252.\n",
    "\n",
    "These metrics measure how well the clustering aligns with the ground\n",
    "truth. Higher precision indicates fewer false positives, and higher\n",
    "recall indicates fewer false negatives. Here, k=2 has the highest recall\n",
    "(0.6252), but k=3 has a reasonable balance between precision and recall.\n",
    "\n",
    "In conclusion, k=3 seems to be a reasonable choice. It has a good\n",
    "silhouette width, and its BCubed Precision and Recall values strike a\n",
    "balance.\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "**6.Evaluation and Comparison**\n",
    "\n",
    "**#1#Classification with comparison criteria**\n",
    "\n",
    "|                | 90%/10% | 90%/10%  |  90%/10%   | 80%/20% | 80%/20%  |  80%/20%   | 70%/30% | 70%/30%  |  70%/30%   |\n",
    "|:-------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "|                |   IG    | IG ratio | Gini Index |   IG    | IG ratio | Gini Index |   IG    | IG ratio | Gini Index |\n",
    "|  **Accuracy**  |  0.872  |  0.8944  |  0.91875   |  0.905  |  0.888   |   0.906    |  0.893  |  0.8939  |   0.911    |\n",
    "| **precision**  |  0.856  |  0.892   |  0.91850   |  0.898  |  0.881   |   0.905    |  0.887  |  0.890   |   0.909    |\n",
    "| **sensitiviy** |  0.856  |  0.891   |  0.91927   |  0.898  |  0.890   |   0.909    |  0.887  |  0.902   |   0.912    |\n",
    "| **specificiy** |  0.919  |  0.945   |   0.9578   |  0.937  |  0.943   |   0.952    |  0.929  |  0.947   |   0.954    |\n",
    "\n",
    "**#1#Clustering with comparison criteria**\n",
    "\n",
    "|                                        | k = 2  | k = 3(BEST) | k = 4    |\n",
    "|----------------------------------------|--------|-------------|----------|\n",
    "| **Average Silhouette width**           | 0.21   | 0.22        | 0.19     |\n",
    "| **total within-cluster sum of square** | 1059.6 | 833.5585    | 762.8895 |\n",
    "| **BCubed precision**                   | 0.0432 | 0.0488      | 0.0505   |\n",
    "| **BCubed recall**                      | 0.6252 | 0.4932      | 0.4101   |\n",
    "| **Visualization**                      | \\-     | \\-          | \\-       |\n",
    "\n",
    "**-note the Visualization In the implementaion of clustering above.-**\n",
    "\n",
    "**Comparison of Classification and Clustering**\n",
    "\n",
    "Classification is better than clustering for our data set because our\n",
    "problem involves predicting kids’ hobbies based on specific attributes,\n",
    "and classification models are designed for precisely this kind of task.\n",
    "In contrast, clustering is useful for discovering natural groupings but\n",
    "doesn’t necessarily assign them to predefined categories. Additionally,\n",
    "clustering might create groups that don’t directly correspond to the\n",
    "desired hobby categories, making interpretation and application more\n",
    "challenging in this context. Hence, the structured nature of\n",
    "classification, where the model is trained on labeled data to predict\n",
    "specific outcomes, makes it more appropriate for our task.\n",
    "\n",
    "The nature of the data types, which include a majority of Boolean and\n",
    "ordinal variables in our dataset influenced the preference for\n",
    "classification over clustering, since classification models,\n",
    "particularly decision trees, are adept at handling these data types. On\n",
    "the other hand, cluster's application might be less straightforward when\n",
    "dealing with categorical and ordinal variables, especially if the goal\n",
    "is to predict predefined categories. making classification more\n",
    "appropriate for our dataset compared to clustering, ‎which may not align\n",
    "with the structured prediction task in our case.\n",
    "\n",
    "The observation of clusters being close to each other when plotted using\n",
    "PAM (Partitioning Around Medoids) while achieving high accuracy in the\n",
    "classification model does influence the preference for classification\n",
    "over clustering. The close proximity and overlap in clusters suggest\n",
    "that the clustering algorithm may not effectively separate the data\n",
    "points based on the given attributes. On the other hand, the high\n",
    "accuracy of the classification model indicates that the chosen\n",
    "classification approach, particularly decision tree models, is\n",
    "successful in accurately predicting the hobby for each data point. This\n",
    "performance difference reinforces the suitability of classification for\n",
    "our dataset, where the goal is to precisely assign specific labels to\n",
    "each observation based on its attributes, and clustering may not provide\n",
    "the necessary separation for effective prediction.\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "**7.Findings**\n",
    "\n",
    "**7.1: Classification Comparision:**\n",
    "\n",
    "In the classification task, the evaluation of 36 metrics revealed\n",
    "distinctive performance patterns across various criteria:\n",
    "\n",
    "1.  Accuracy:\n",
    "\n",
    "Gini 90-10 had the highest accuracy at 91.88%, closely followed by Gini\n",
    "70-30 at 91.06%.\n",
    "\n",
    "Information Gain 80-20 also displayed strong accuracy at 90.57%, and\n",
    "Gini 80-20 had an accuracy of 90.62%.\n",
    "\n",
    "Ratio models had slightly lower accuracies ranging from 89.44% (Ratio\n",
    "90-10) to 88.79% (Ratio 80-20 and 70-30).\n",
    "\n",
    "Information Gain 70-30 and Gini 80-20 closely followed with accuracies\n",
    "around 89.32% and 90.62% respectively.\n",
    "\n",
    "Information Gain 90-10 displayed a lower accuracy at 87.21%.\n",
    "\n",
    "1.  Sensitivity:\n",
    "\n",
    "Gini models consistently showed high sensitivity ranging from 91.19%\n",
    "(Gini 70-30) to 91.93% (Gini 90-10).\n",
    "\n",
    "Ratio models followed with sensitivity levels around 89.15% to 90.25%.\n",
    "\n",
    "Information Gain models demonstrated slightly lower sensitivity, ranging\n",
    "from 85.66% (Information Gain 90-10) to 89.83% (Information Gain 80-20).\n",
    "\n",
    "1.  Specificity:\n",
    "\n",
    "Gini models showcased strong specificity, reaching from 95.15% (Gini\n",
    "80-20) to 95.79% (Gini 90-10).\n",
    "\n",
    "Ratio models exhibited specificity between 94.26% (Ratio 80-20) and\n",
    "94.66% (Ratio 70-30).\n",
    "\n",
    "Information Gain models displayed specificity levels around 91.91%\n",
    "(Information Gain 90-10) to 93.78% (Information Gain 80-20).\n",
    "\n",
    "1.  Precision:\n",
    "\n",
    "Gini models maintained high precision, ranging from 90.46% (Gini 80-20)\n",
    "to 91.85% (Gini 90-10).\n",
    "\n",
    "Ratio models demonstrated precision between 88.11% (Ratio 80-20) and\n",
    "89.19% (Ratio 90-10).\n",
    "\n",
    "Information Gain models exhibited precision around 85.66% (Information\n",
    "Gain 90-10) to 89.83% (Information Gain 80-20).\n",
    "\n",
    "Overall, Gini models consistently showcased strong performance across\n",
    "most metrics, especially in sensitivity and specificity, followed\n",
    "closely by Ratio models. Information Gain models, while competitive in\n",
    "accuracy, displayed slightly lower sensitivity and specificity compared\n",
    "to Gini and Ratio models.\n",
    "\n",
    "**Analyzing and Selecting the Best Trees from Each Method**\n",
    "\n",
    "1.  Gini Index:\n",
    "\n",
    "Best Tree: Gini 90-10\n",
    "\n",
    "-   Accuracy: 91.88%\n",
    "\n",
    "-   Sensitivity: 91.93%\n",
    "\n",
    "-   Specificity: 95.79%\n",
    "\n",
    "-   Precision: 91.85%\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The Gini 90-10 tree outperforms others, providing the highest accuracy\n",
    "and well-balanced sensitivity and specificity. This tree exhibits\n",
    "superior predictive power, making it an optimal choice for accurately\n",
    "categorizing children’s hobbies.\n",
    "\n",
    "The most influential attributes in predicting hobbies within the Gini\n",
    "Index Decision Tree (90/10) are engagement in sports activities\n",
    "(‘Career_sprt’), involvement in arts hobbies (‘Fant_arts’), and the\n",
    "level of arts activity (‘Time_art’).\n",
    "\n",
    "Confusion Matrix for Gini 90-10:\n",
    "\n",
    "                      Predicted Positive    Predicted Negative\n",
    "\n",
    "Actual Positive            True Positive (91.93%)         False Negative\n",
    "(8.07%)\n",
    "\n",
    "Actual Negative            False Positive (4.21%)         True Negative\n",
    "(95.79%)\n",
    "\n",
    "1.  Ratio:\n",
    "\n",
    "Best Tree: Ratio 90-10\n",
    "\n",
    "-   Overall Accuracy: 89.44%\n",
    "\n",
    "-   Average Sensitivity: 89.15%\n",
    "\n",
    "-   Average Specificity: 94.66%\n",
    "\n",
    "-   Average Precision: 89.19%\n",
    "\n",
    "Explanation: Among Ratio trees, the Ratio 90-10 tree stands out with\n",
    "competitive accuracy, sensitivity, and specificity. It strikes a balance\n",
    "between identifying positive cases and correctly predicting negative\n",
    "cases, making it a reliable choice for predicting children’s hobbies.\n",
    "\n",
    "The primary attributes influencing hobby predictions in the Gain Ratio\n",
    "Decision Tree (90%/10%) are ‘Career_sprt,’ ‘Fant_arts,’ ‘Won_arts,’\n",
    "‘Olympiad_Participation,’ ‘Grasp_pow,’ ‘Time_art,’ and ‘Act_sprt.’\n",
    "\n",
    "Confusion Matrix for Ratio 90-10:\n",
    "\n",
    "                      Predicted Positive    Predicted Negative\n",
    "\n",
    "Actual Positive            True Positive (89.15%)         False Negative\n",
    "(10.85%)\n",
    "\n",
    "Actual Negative            False Positive (5.34%)         True Negative\n",
    "(94.66%)\n",
    "\n",
    "1.  Information Gain:\n",
    "\n",
    "Best Tree: Information Gain 80-20\n",
    "\n",
    "-   Accuracy: 90.57%\n",
    "\n",
    "-   Sensitivity: 89.83%\n",
    "\n",
    "-   Specificity: 93.78%\n",
    "\n",
    "-   Precision: 89.83%\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The Information Gain 80-20 tree exhibits strong accuracy and\n",
    "well-balanced sensitivity and specificity. It effectively utilizes\n",
    "information gain to predict children’s hobbies, making it a favorable\n",
    "choice within the Information Gain method.\n",
    "\n",
    "The most influential attributes in predicting hobbies within the\n",
    "Information Gain Decision Tree (80/20) include engagement in sports\n",
    "activities (‘Career_sprt’), involvement in arts hobbies (‘Fant_arts’),\n",
    "and the level of arts activity (‘Time_art’).\n",
    "\n",
    "Confusion Matrix for Information Gain 80-20:\n",
    "\n",
    "                      Predicted Positive    Predicted Negative\n",
    "\n",
    "Actual Positive            True Positive (89.83%)          False\n",
    "Negative (10.17%)\n",
    "\n",
    "Actual Negative            False Positive (6.22%)          True Negative\n",
    "(93.78%)\n",
    "\n",
    "**Overall Analysis:**\n",
    "\n",
    "\\- The Gini 90-10 tree excels in accuracy, sensitivity, and specificity\n",
    "within the Gini Index method.\n",
    "\n",
    "\\- The Ratio 90-10 tree demonstrates competitive performance,\n",
    "particularly in accuracy and specificity within the Ratio method.\n",
    "\n",
    "\\- The Information Gain 80-20 tree showcases strong accuracy and\n",
    "balanced sensitivity and specificity within the Information Gain method.\n",
    "\n",
    "The selection of the best tree depends on specific priorities. If a\n",
    "well-balanced model is crucial, Gini 90-10 stands out. For a balanced\n",
    "Ratio model, Ratio 90-10 is favorable. Information Gain 80-20 offers\n",
    "strong accuracy and balance within the Information Gain method.\n",
    "\n",
    "Best Tree Across All Methods:\n",
    "\n",
    "Best Tree: Gini 90-10\n",
    "\n",
    "Accuracy: 91.88%\n",
    "\n",
    "Sensitivity: 91.93%\n",
    "\n",
    "Specificity: 95.79%\n",
    "\n",
    "Precision: 91.85%\n",
    "\n",
    "The Gini 90-10 tree stands out as the best-performing tree across all\n",
    "methods, displaying superior accuracy, sensitivity, specificity, and\n",
    "precision. It excels in accurately predicting children’s hobbies, making\n",
    "it the optimal choice among the evaluated trees.\n",
    "\n",
    "The Gini index with a 90-10 attribute selection measure has emerged as\n",
    "the most effective solution in our classification model for predicting\n",
    "children’s hobbies. This particular attribute selection method\n",
    "prioritizes the most relevant features, ensuring that the decision tree\n",
    "focuses on the attributes that contribute significantly to the\n",
    "classification accuracy. The practical implications of this solution are\n",
    "substantial, as it prioritizes certain attributes, such as Olympiad\n",
    "participation, scholarship status, and favorite subject. The model\n",
    "offers valuable insights for parents and educators in guiding children\n",
    "toward activities aligned with their interests and strengths. Compared\n",
    "to other attribute selection measures, it leads to more accurate\n",
    "predictions and, consequently, more tailored recommendations for\n",
    "children’s hobbies. This refined approach enhances the model’s practical\n",
    "utility, making it a valuable tool for assisting parents in fostering an\n",
    "environment where children can explore and excel in activities that\n",
    "truly resonate with their individual preferences.\n",
    "\n",
    "**7.2: the best partitioning method for clustering:**\n",
    "\n",
    "According to our research and attempt to cluster using k-means algorithm\n",
    "we figured that it is not applicable to our data because it relies on\n",
    "the Euclidean distance metric to measure the similarity between data\n",
    "points. However, even after Encoding and its application to categorical\n",
    "data pose significant challenges. Categorical variables often lack a\n",
    "meaningful numerical representation; for instance, taking the mean of\n",
    "categories like the feature \"favorite subject,\" aka \"Fav_sub\" (even\n",
    "after encoding), might not have any practical interpretation. And the\n",
    "distances calculated in the algorithm may not reflect the true\n",
    "dissimilarities between categorical values. The encoding process itself\n",
    "introduces artificial numerical relation that may mislead the algorithm.\n",
    "Moreover, k-means relies on the minimization of Euclidean distances,\n",
    "which might not accurately capture the dissimilarity structure in\n",
    "categorical data. Categorical variables inherently exhibit discrete and\n",
    "non-ordinal characteristics that are not well-suited for the continuous\n",
    "and linear assumptions of k-means. Alternative clustering techniques,\n",
    "specifically designed for categorical data, such as partitioning around\n",
    "medoids, are more appropriate for capturing the intrinsic patterns and\n",
    "relationships in categorical data sets. \\[7\\]\n",
    "\n",
    "**7.3: Clustering Comparison:**\n",
    "\n",
    "In assessing and comparing the performance of 12 metrics in the\n",
    "clustering task, we observe notable variations in their effectiveness\n",
    "across different values of K. For K=2, the Average Silhouette Width of\n",
    "0.21 indicates a moderate level of separation between clusters, while\n",
    "the BCubed precision and recall highlight challenges in precisely\n",
    "classifying data points. Moving to K=3, the Average Silhouette Width\n",
    "slightly improves to 0.22, suggesting enhanced cluster distinctiveness.\n",
    "The BCubed metrics, precision, and recall, also exhibit improvements,\n",
    "indicating a more balanced and accurate clustering. However, K=4\n",
    "witnesses a decline in the Average Silhouette Width to 0.19, reflecting\n",
    "a potential decrease in separation between clusters. Despite this, the\n",
    "BCubed metrics continue to demonstrate reasonable precision and recall\n",
    "values, emphasizing the resilience of the clustering patterns. Overall,\n",
    "these metrics collectively offer insights into the trade-offs between\n",
    "cluster compactness and separation at different K values, guiding the\n",
    "selection of K=3 as the optimal choice for capturing meaningful\n",
    "clustering patterns within the data set. The fact that the actual class\n",
    "labels in our data set consist of three distinct classes aligns with the\n",
    "findings from the clustering analysis, particularly at K=3. This\n",
    "convergence strengthens the interpretation of the results, as it\n",
    "indicates that the clustering algorithm successfully identified and\n",
    "differentiated the inherent structure of the data. The agreement between\n",
    "the actual class labels and the clusters formed by the algorithm at K=3\n",
    "suggests that the clustering patterns are meaningful and aligned with\n",
    "the underlying characteristics of the data set. It reinforces the\n",
    "relevance of using K=3 as the optimal choice, as it aligns with the\n",
    "inherent class structure in the data set, providing a more accurate\n",
    "representation of the underlying patterns in the children’s hobbies.\n",
    "\n",
    "The chosen K holds significant implications for addressing the problem.\n",
    "In the context of guiding children towards suitable hobbies, it plays a\n",
    "pivotal role in optimizing the effectiveness of our model for parents in\n",
    "making informed choices. If K were set higher than 3, the categorization\n",
    "would lack the depth needed for insightful distinctions among\n",
    "preferences. On the other hand, if K were lower than 3, the model’s\n",
    "accuracy could be compromised, as one or more class labels may not be\n",
    "adequately defined within the clusters. This deficiency would adversely\n",
    "impact the precision of predictions, hindering the model’s ability to\n",
    "offer precise guidance to parents. By settling on K=3, we strike a\n",
    "balance that ensures meaningful and well-defined clusters, providing\n",
    "parents with a reliable tool to navigate and understand their child’s\n",
    "inclinations effectively, offering a comprehensive categorization that\n",
    "aligns with the inherent structure of the dataset.\n",
    "\n",
    "**7.4: Clustering Interpretation:**\n",
    "\n",
    "``` {r}\n",
    "# Create a temporary data frame for clustering results\n",
    "clustered_data <- data.frame(Hobby_Data3, cluster = pam.result$clustering)\n",
    "\n",
    "# Show all data points in each cluster\n",
    "for (i in unique(pam.result$clustering)) {\n",
    "  cat(\"Cluster\", i, \":\\n\")\n",
    "  print(clustered_data[clustered_data$cluster == i, -ncol(clustered_data)])\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "```\n",
    "\n",
    "**Cluster 1:**\n",
    "\n",
    "Based on the data points, the students in cluster 1 are primarily\n",
    "interested in academics. They are more likely to have participated in\n",
    "Olympiads, have received scholarships, and they favor challenging\n",
    "courses, since there are 98 out of 150 of them chose science as their\n",
    "favorite subject. They also tend to spend more time on academics than on\n",
    "sports or arts.\n",
    "\n",
    "The predicted hobby for students in cluster 1 is **academics**. This is\n",
    "supported by the fact that they have the following characteristics:\n",
    "\n",
    "-   **High academic achievement:** They have participated in Olympiads,\n",
    "    received scholarships, and favor challenging courses.\n",
    "\n",
    "-   **Strong focus on academics:** They spend more time on academics\n",
    "    than on sports or arts.\n",
    "\n",
    "-   Within this cluster, there is a lower interest or participation in\n",
    "    sports activities (Career_sprt), but there may be a significant\n",
    "    focus on winning art competitions (Won_arts) and enjoying fantasy\n",
    "    arts (Fant_arts).\n",
    "\n",
    "**Cluster 2:**\n",
    "\n",
    "Based on the data points, the students in cluster 2 are primarily\n",
    "interested in sports. They are more likely to have played sports in\n",
    "school and to be involved in sports activities in general. They also\n",
    "tend to spend more time on sports than on academics or arts.\n",
    "\n",
    "The predicted hobby for students in cluster 2 is **sports**. This is\n",
    "supported by the fact that they have the following characteristics:\n",
    "\n",
    "-   **High involvement in sports:** They played sports in school and are\n",
    "    involved in sports activities now. This implies a strong interest\n",
    "    and participation in sports activities (Olympiad_Participation,\n",
    "    Act_sprt).\n",
    "\n",
    "-   **Strong focus on sports:** They spend more time on sports than on\n",
    "    academics or arts (Time_sprt).\n",
    "\n",
    "-   There may be lower interest in fantasy arts (Fant_arts) within this\n",
    "    cluster.\n",
    "\n",
    "**Cluster 3:**\n",
    "\n",
    "Based on the data points, the students in cluster 3 are primarily\n",
    "interested in arts. They tend to spend more time on arts than on\n",
    "academics or sports.\n",
    "\n",
    "The predicted hobby for students in cluster 3 is **arts**. This is\n",
    "supported by the fact that they have the following characteristics:\n",
    "\n",
    "-   **Strong focus on arts:** They tend to have a strong interest in\n",
    "    creating fantasy paintings (Fant_arts), potentially winning art\n",
    "    competitions (Won_arts), and dedicating time to artistic pursuits\n",
    "    (Time_art).\n",
    "\n",
    "-   There may be lower interest or participation in sports activities\n",
    "    (Career_sprt, Olympiad_Participation, Act_sprt) within this cluster.\n",
    "\n",
    "In conclusion, this analysis involved examination of clusters within our\n",
    "dataset, with a focus on understanding the shared characteristics of\n",
    "each cluster by considering variable importance ranks.This clustering\n",
    "approach and subsequent interpretation offer valuable insights into the\n",
    "diverse preferences and tendencies within our data set, understanding\n",
    "these clusters can be benificail in personalizing educational or\n",
    "recreational experiences based on individual interests and inclinations.\n",
    "\n",
    "**7.5: Comparison of Classification and Clustering:** Classification is\n",
    "better than clustering for our data set because our problem involves\n",
    "predicting kids’ hobbies based on specific attributes, and\n",
    "classification models are designed for precisely this kind of task. In\n",
    "contrast, clustering is useful for discovering natural groupings but\n",
    "doesn’t necessarily assign them to predefined categories. Additionally,\n",
    "clustering might create groups that don’t directly correspond to the\n",
    "desired hobby categories, making interpretation and application more\n",
    "challenging in this context. Hence, the structured nature of\n",
    "classification, where the model is trained on labeled data to predict\n",
    "specific outcomes, makes it more appropriate for our task.\n",
    "\n",
    "The nature of the data types, which include a majority of Boolean and\n",
    "ordinal variables in our dataset influenced the preference for\n",
    "classification over clustering, since classification models,\n",
    "particularly decision trees, are adept at handling these data types. On\n",
    "the other hand, cluster's application might be less straightforward when\n",
    "dealing with categorical and ordinal variables, especially if the goal\n",
    "is to predict predefined categories. making classification more\n",
    "appropriate for our dataset compared to clustering, ‎which may not align\n",
    "with the structured prediction task in our case.\n",
    "\n",
    "The observation of clusters being close to each other when plotted using\n",
    "PAM (Partitioning Around Medoids) while achieving high accuracy in the\n",
    "classification model does influence the preference for classification\n",
    "over clustering. The close proximity and overlap in clusters suggest\n",
    "that the clustering algorithm may not effectively separate the data\n",
    "points based on the given attributes. On the other hand, the high\n",
    "accuracy of the classification model indicates that the chosen\n",
    "classification approach, particularly decision tree models, is\n",
    "successful in accurately predicting the hobby for each data point. This\n",
    "performance difference reinforces the suitability of classification for\n",
    "our dataset, where the goal is to precisely assign specific labels to\n",
    "each observation based on its attributes, and clustering may not provide\n",
    "the necessary separation for effective prediction.\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "**8.References(Using IEEE format)**\n",
    "\n",
    "\\[1\\]SagarDhandare, “What Is Encoding? And Its Importance in Data\n",
    "Science!,” Medium, Mar. 28, 2022.\n",
    "https://medium.datadriveninvestor.com/what-is-encoding-and-its-importance-in-data-science-6a2b0cce8e8e\n",
    "\n",
    "\\[2\\]“6.3. Preprocessing data — scikit-learn 0.23.1 documentation,”\n",
    "scikit-learn.org.\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html#normalization\n",
    "\n",
    "\\[3\\]https://www.facebook.com/jason.brownlee.39, “Feature Selection with\n",
    "the Caret R Package,” Machine Learning Mastery, Aug. 22, 2019.\n",
    "https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/\n",
    "\n",
    "\\[4\\]“RPubs - Decision Tree Using (Information Gain),” rpubs.com.\n",
    "https://rpubs.com/SameerMathur/DT_InformationGain_CCDefault (accessed\n",
    "Nov. 30, 2023).\n",
    "\n",
    "\\[5\\]“RPubs - Decision Tree (Gini),” rpubs.com.\n",
    "https://rpubs.com/SameerMathur/DT_Gini_CCDefault (accessed Dec. 01,\n",
    "2023).\n",
    "\n",
    "\\[6\\]“RPubs - Data Mining: Classification with Decision Trees,”\n",
    "rpubs.com. https://rpubs.com/kjmazidi/195428 (accessed Nov. 30, 2023).\n",
    "\n",
    "\\[7\\]“Clustering binary data with K-Means (should be avoided),”\n",
    "www.ibm.com, Apr. 16, 2020.\n",
    "https://www.ibm.com/support/pages/clustering-binary-data-k-means-should-be-avoided\n",
    "\n",
    "\\[8\\]A. Kassambara, “Determining The Optimal Number Of Clusters: 3 Must\n",
    "Know Methods - Datanovia,” Datanovia, 2018.\n",
    "https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/\n",
    "\n",
    "\\`"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
